<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Vanessa&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Vanessa&#039;s blog🎀"><meta name="msapplication-TileImage" content="/img/my_favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Vanessa&#039;s blog🎀"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Vanessa&#039;s blog"><meta property="og:url" content="https://jennyvanessa.github.io/"><meta property="og:site_name" content="Vanessa&#039;s blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jennyvanessa.github.io/img/og_image.png"><meta property="article:author" content="Vanessa Ni"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://jennyvanessa.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jennyvanessa.github.io"},"headline":"Vanessa's blog","image":["https://jennyvanessa.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Vanessa Ni"},"publisher":{"@type":"Organization","name":"Vanessa's blog","logo":{"@type":"ImageObject","url":{"text":"Just be here now🎇"}}},"description":""}</script><link rel="icon" href="/img/my_favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><script type="text/javascript" color="255,255,255" opacity="0.7" zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Just be here now🎇</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-20T11:28:02.000Z" title="2023/3/20 19:28:02">2023-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-20T11:37:37.759Z" title="2023/3/20 19:37:37">2023-03-20</time></span><span class="level-item">4 minutes read (About 611 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/20/2303201927/">Paper | CoordFill, Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying | AAAI2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying</code></p>
</li>
<li><p>Keyword：<strong>Adaptive neural networks</strong></p>
</li>
<li><p>Idea：Parameterized Coordinate Querying</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2023年3月15号arXiv submitted，AAAI2023 accepted。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.08524">CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying (arxiv.org)</a></li>
<li>Code，作者还没更新repo。<a target="_blank" rel="noopener" href="https://github.com/NiFangBaAGe?tab=repositories">NiFangBaAGe (NiFangBaAGe) &#x2F; Repositories (github.com)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>高分辨率图像修复现存的问题：</p>
<ul>
<li>高分辨率图像需要较大的感受野，造成更多的计算量。</li>
<li>encoder-decoder架构会同时<strong>重建背景像素</strong>（非受损区域），作者认为这样会降低效率。</li>
</ul>
<p>贡献点：</p>
<ul>
<li>首次将continuous implicit representation引入高分辨率图像修复。</li>
<li>在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.02992">Spatially-Adaptive Pixelwise Networks for Fast Image Translation (arxiv.org)</a>的基础上，提出基于注意力快速傅里叶卷积（<strong>Attentional FFC</strong>）的参数生成网络。并且<strong>只针对掩膜区域进行重建</strong>。</li>
</ul>
<p><img src="/2023/03/20/2303201927/1.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/20/2303201927/2.png"></p>
<ul>
<li><p>先将高分辨率图像（e.g. 1024×1024）双线性下采样到低分辨率空间（e.g. 256×256）。Encoder为三层卷积层，将输入图片特征继续下采样，然后利用六个基于注意力的傅里叶卷积块生成最终的逐像素MLPs参数。</p>
</li>
<li><p>利用一个简单的线性映射，只选取孔洞区域的映射，并将目标分辨率作为条件输入使网络能够对于分辨率敏感。最后利用最近邻插值将逐像素MLPs参数上采样到高分辨率空间，加入带有高频信息的正弦信号位置编码，仅生成代填补区域像素，最终直接贴回原图。</p>
</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p><img src="/2023/03/20/2303201927/3.png"></p>
<ul>
<li>主打一个快，但是增益是之前CVPR21的I2I提出的，把该方法迁移到了Inpainting任务上而已。</li>
</ul>
<p><img src="/2023/03/20/2303201927/4.png"></p>
<ul>
<li><p>可能对于高分辨率图像修复的<strong>实际应用部署</strong>会有很大的帮助。</p>
</li>
<li><p>在人脸图像上也做了实验，但是与HiFill保持训练一致，训练mask只采取了25%左右的缺失区域，很难认定这是高分辨率图像修复任务。</p>
</li>
</ul>
<p><img src="/2023/03/20/2303201927/5.png"></p>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><blockquote>
<p>灌水痕迹明显的一篇AAAI。</p>
<p>但可能是因为作者是工业界的，用了八卡a100训练这样的模型，方便实际中的高分辨率图像修复部署吧。科研上的价值不大。</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-20T11:17:32.000Z" title="2023/3/20 19:17:32">2023-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-20T11:26:34.196Z" title="2023/3/20 19:26:34">2023-03-20</time></span><span class="level-item">4 minutes read (About 541 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/20/2303201917/">Paper | Spatially-Adaptive Pixelwise Networks for Fast Image Translation | CVPR2021</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Spatially-Adaptive Pixelwise Networks for Fast Image Translation</code></li>
<li>Keyword：<strong>Adaptive neural networks</strong>，Trainable efficient &amp; Gan-based I2I</li>
</ul>
<blockquote>
<p>Trainable efficient image transformations：Computationally heavy<br>inference is performed at extreme low-resolution, while the<br>high-resolution synthesis is comparatively lightweight. </p>
<p>计算量大的推理在极低分辨率下执行，而高分辨率合成更轻量级。</p>
</blockquote>
<ul>
<li>Idea：Pixel-specific Lightweight MLPs</li>
<li>Source<ul>
<li>Paper，2020年12月arXiv submitted，CVPR2021 accepted。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.02992">Spatially-Adaptive Pixelwise Networks for Fast Image Translation (arxiv.org)</a></li>
<li>Code，<a target="_blank" rel="noopener" href="https://tamarott.github.io/ASAPNet_web/">ASAPNet (tamarott.github.io)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>存在的问题：</p>
<ul>
<li>现有的I2I方法随着性能的提升，推理时间也不断增加。</li>
</ul>
<p><img src="/2023/03/20/2303201917/1.png"></p>
<p>本文的贡献：</p>
<ul>
<li>在不牺牲性能的同时，降低运行时间，提升推理速度。</li>
</ul>
<p><img src="/2023/03/20/2303201917/2.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/20/2303201917/3.png"></p>
<p>直觉上直接使用MLP预测效果会很差，文章提出了三个组件使网络表达能力增强。</p>
<ul>
<li><strong>Spatially-varying Parameters</strong>：与CNN的参数共享不同，本方法针对每个像素点的MLP参数是不同的（spatial-adaptive、spatial-varying）。逐像素预测可以并行计算，因为像素与像素之间独立，因此推理速度大幅度提升。</li>
<li><strong>Sinusoid Positional Encoding</strong>：除了input pixel values以外，还将像素空间位置以不同频率的正弦信号编码，其频率高于上采样因子，以生成高频图像细节。</li>
<li><strong>Input-adaptive Parameters</strong>：在低分辨率空间预测参数，对于每张图预测的逐像素MLPs参数均不同。</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>主打一个快，且性能没有显著下降。</li>
</ul>
<p><img src="/2023/03/20/2303201917/4.png"></p>
<ul>
<li><strong>MLPs一般只能学习到低频细节</strong>。在连续隐式表达中（<strong>神经渲染方向</strong>），MLPs表征能力不是很强，就像消融实验中的没有位置编码的模型生成结果一样。所以本文牺牲了极大的训练内存来换取速度。</li>
</ul>
<p><img src="/2023/03/20/2303201917/5.png"></p>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><ul>
<li>听师兄说，这是20年、21年CVPR比较热门的（灌水）方法。</li>
<li>后续会follow一篇在23年利用该方法发表（灌水）AAAI23的高分辨率图像修复方法。</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-17T06:05:11.000Z" title="2023/3/17 14:05:11">2023-03-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-17T06:15:32.745Z" title="2023/3/17 14:15:32">2023-03-17</time></span><span class="level-item">5 minutes read (About 682 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/17/2303171405/">Growth | Be a Techno-Optimist</a></p><div class="content"><p>All human progress is about overcoming an obstacle. <strong>From the wheel to the internet</strong>, we have discovered and invented our way out of all sorts of trouble. The story of science and technology is, in the main, one of making our lives easier.</p>
<p>Instead of talking out of both sides of our mouths, perhaps it is time that we were appreciated just how much we need technology, and how far it has helped us along. That is exactly what philosopher John Danaher does in his recent paper, Techno-optimism: an Analysis, an Evaluation and a Modest Defense.</p>
<p>Obviously, technology is not perfect. Smart phone addiction does exist, environmental destruction is happening, and we are each seeing a rapid, abrupt uprooting of how society has operated for millennia. If we say we’re “techno-optimists,” we are not saying that we are blind to technology’s problems. <strong>Optimism is not fanaticism</strong>.</p>
<p>Instead, as Danaher argues, optimism is defined by three elements. <strong>First</strong>, optimists believe “the good prevails over the bad by some distance, with that distance varying depending on the strength of the optimistic stance.” So, in terms of technology, it means <strong>the good outweighs the bad</strong>.</p>
<p><strong>Second</strong>, optimism tends to associate with an “<strong>affirmation of improvement</strong>.” The year 2022 is a better time to live than 1880   —   or even 1980.</p>
<p><strong>Third</strong>, optimists (and pessimists, for that matter) must believe that we can actually <strong>measure</strong> “good” as a value to track. We can point to this or that technology and say, “These are examples of good things that could only be caused by technological improvement.”</p>
<p>According to Danaher, in order to properly justify and rationalize techno-optimism, we must do three things: <strong>Establish values, determine facts, and evaluate</strong>.</p>
<p><strong>Establish values</strong>. First, we must establish certain values as being “good.” For instance, a techno-optimist “might argue that it is wonderful that people have more disposable income and a richer set of consumer goods and services from which to choose.”</p>
<p><strong>Determine facts</strong>. Once we have established these values, then we have to present those facts that support the claim that technology provides them.</p>
<p><strong>Evaluate</strong>. <strong>We need to present the facts that defend values, but we also need to acknowledge facts that contradict those values as well</strong>. As mentioned above, technology does have its problems. It can impact our mental health, it ravages the environment, and it drastically upends what being human has always meant. The techno-optimist is the one who believes the good of technology outweighs the bad.</p>
<p>There are two major critiques of techno-optimism that Danaher addresses.</p>
<p>First, the “treadmill critique” argues that technology won’t constantly make the world better. We have become so accustomed to technology that we no longer appreciate it as “good” but rather expect it as the norm.</p>
<p>Danaher counters by suggesting that there exists some “values that are not subject to baseline adaptation.” He cites “longer lives, fewer life-threatening illness, and more equality of opportunity” as examples of “goods” that will always be good, regardless of how accustomed we are to them.</p>
<p>Second, the “unsustainability critique” is the idea that if “optimism depends on present or continued economic growth, it also depends on the continued technological exploitation of natural resources. All natural resources are finite and have some upper limit of exploitability.”</p>
<p>Danaher’s response is that “technology is becoming less exploitative over time.” As a technology improves, then growth “decouples” from exploitation.</p>
<p>You do not have to adopt a starry-eyed “technology-will-save-humanity” viewpoint to be a techno-optimist. It’s perfectly reasonable to suggest that there are many existing problems with technology, and that it, alone, is insufficient for good to prevail.</p>
<p>Instead, we might sympathize with Danaher’s “modest techno-optimism.” According to this view, “we have the power to create the right institutions for generating, selecting, and creating material technologies, and acting on that belief in a cautious and sensible manner can make it more likely that the good will prevail over the bad.”</p>
<p><strong>It’s a kind of techno-optimism that perhaps needs a bit of human optimism, too</strong>.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-15T08:51:48.000Z" title="2023/3/15 16:51:48">2023-03-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-15T09:04:38.458Z" title="2023/3/15 17:04:38">2023-03-15</time></span><span class="level-item">7 minutes read (About 1052 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/15/2303151651/">Trend | GPT-4 —— The winner takes all</a></p><div class="content"><h1 id="Image-and-Text-Multimodal"><a href="#Image-and-Text-Multimodal" class="headerlink" title="Image and Text Multimodal"></a>Image and Text Multimodal</h1><blockquote>
<p>We report the development of GPT-4, a <strong>large-scale</strong>, <strong>multimodal model</strong> which can <strong>accept image and text inputs and produce text outputs</strong>.</p>
</blockquote>
<ul>
<li>模型底层逻辑还是image+text input（融入多模态元素，更唬人一些？），但还是文本outputs（不过听说chatgpt plus版本已经可以有image output了，怀疑是一些命令的组合？就类似于上一篇微软刚提出的Vision Chatgpt的方式一样，将视觉模型作为tool模型，large-scale语言预训练作为agent模型）。</li>
<li><strong>支持输入更多的tokens</strong>（更个性化，更方便定制了，更task-specific了）</li>
<li>加了一些VQA的性能对比。</li>
</ul>
<h1 id="Professional-and-Academic-Benchmarks"><a href="#Professional-and-Academic-Benchmarks" class="headerlink" title="Professional and Academic Benchmarks"></a>Professional and Academic Benchmarks</h1><blockquote>
<p>While less capable than humans in many real-world scenarios, GPT-4 exhibits <strong>human-level performance on various professional and academic benchmark</strong>s, including passing a simulated bar exam with a score around the top 10% of test takers. </p>
</blockquote>
<ul>
<li>professional benchmarks</li>
</ul>
<blockquote>
<p>乱杀应试教育界，秒杀多少普通人。</p>
<p>AI for science提上日程吧，早日研究，然后自我替代（开玩笑，不过很期待这一天）。</p>
</blockquote>
<p><img src="/2023/03/15/2303151651/1.png"></p>
<blockquote>
<p>这GRE、leetcode水平，感觉我自己都要花点时间才能达到呢。</p>
</blockquote>
<ul>
<li><p>academic benchmark</p>
<p>已经叫做<strong>benchmark-specific tuning</strong>了，面向任务的DL调参侠瑟瑟发抖。</p>
<p><img src="/2023/03/15/2303151651/2.png"></p>
</li>
</ul>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><p><img src="/2023/03/15/2303151651/3.png"></p>
<ul>
<li><p>这个part让我觉得，训练一个大模型需要好多方面的协调，包括</p>
<ul>
<li><strong>Pretraining</strong><ul>
<li>Compute cluster scaling</li>
<li><strong>Data</strong></li>
<li>Distributed training infrastructure</li>
<li>Hardware correctness</li>
<li>Optimization &amp; architecture</li>
<li><strong>Training run babysitting</strong></li>
</ul>
</li>
<li><strong>Long context</strong><ul>
<li>Long context research</li>
<li>Long context kernels</li>
</ul>
</li>
<li><strong>Vision</strong><ul>
<li>Architecture research</li>
<li>Compute cluster scaling</li>
<li>Distributed training infrastructure</li>
<li>Hardware correctness</li>
<li>Data</li>
<li>Alignment data</li>
<li>Training run babysitting</li>
<li>Deployment &amp; post-training</li>
</ul>
</li>
<li><strong>Reinforcement Learning &amp; Alignment</strong><ul>
<li><strong>Dataset contributions</strong></li>
<li><strong>Data infrastructure</strong></li>
<li>ChatML format</li>
<li><strong>Model safety</strong></li>
<li>Refusals</li>
<li>Foundational RLHF and InstructGPT work</li>
<li>Flagship training runs</li>
<li>Code capability</li>
</ul>
</li>
<li><strong>Evaluation &amp; analysis</strong><ul>
<li>OpenAI Evals library</li>
<li>Model-graded evaluation infrastructure</li>
<li>Acceleration forecasting</li>
<li>ChatGPT evaluations</li>
<li>Capability evaluations</li>
<li>Coding evaluations</li>
<li>Real-world use case evaluations</li>
<li>Contamination investigations</li>
<li>Instruction following and API evals</li>
<li>Novel capability discovery</li>
<li>Vision evaluations</li>
<li>Economic impact evaluation</li>
<li>Non-proliferation, international humanitarian law &amp; national security red teaming</li>
<li>Overreliance analysis</li>
<li>Privacy and PII evaluations</li>
<li>Safety and policy evaluations</li>
<li><strong>OpenAI adversarial testers</strong></li>
<li>System card &amp; broader impacts analysis</li>
</ul>
</li>
<li><strong>Deployment</strong><ul>
<li>Inference research</li>
<li>GPT-4 API &amp; ChatML deployment</li>
<li>GPT-4 web experience</li>
<li>Inference infrastructure</li>
<li>Reliability engineering</li>
<li>Trust &amp; safety engineering</li>
<li>Trust &amp; safety monitoring and response</li>
<li>Trust &amp; safety policy</li>
<li>Deployment compute</li>
<li>Product management</li>
</ul>
</li>
<li><strong>Additional contributions</strong><ul>
<li><strong>Blog post &amp; paper content</strong></li>
<li>Communications</li>
<li>Compute allocation support</li>
<li>Contracting, revenue, pricing, &amp; finance support</li>
<li>Launch partners &amp; product operations</li>
<li>Legal</li>
<li>Security &amp; privacy engineering</li>
<li>System administration &amp; on-call support</li>
</ul>
</li>
</ul>
</li>
<li><p>比较费人的小部门就是data和training部分（标粗显示的部分），然后就是领域专家给反馈（adversarial testers）。</p>
</li>
<li><p>算法部分Pretraining+long context+Vision+RL，测试部署Evaluation+deployment，以及后期各种市场、产品，都缺一不可，都很关键啊。不过能看到AI产品能够有今天，也是十分欣慰了，以前的AI都停留在弱弱弱弱AI的层面吧，好处是觉得自己学的东西真的能改变世界，学科真的有技术爆炸式的飞跃进展，坏处是自己好像没什么用处了（美滋滋，不过发展的尽头，不都是要被替代的吗？语言、教育、设计、律师、计算机、金融各行各业，不论是专业性的，还是需要想象力的艺术生成，好像AI在某种程度上已经击败了90%的人类了吧）。</p>
</li>
<li><p>3年前的自己还很有信念的All in AI，坚信Deep Learning，距离通用AI的出现或许真的不远咯。</p>
</li>
<li><p>目前的AI变强了，但还是<strong>辅助人类办公，提升效率的帮手</strong>，距离完全代替人类还有很长的路要走（甚至真正的商业化都比较麻烦？）。愈发认为，人类的情感、情绪价值，在当下变得更为宝贵、更难以替代一些。</p>
</li>
<li><p><strong>未来究竟是理性的胜利、还是感性的胜利，是机器的胜利、还是人类的胜利呢。</strong>如果有生之年能够见证的话，还挺让人期待的。</p>
</li>
<li><p><strong>不过当下，打不过就加入嘛！</strong></p>
</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><p><a target="_blank" rel="noopener" href="https://openai.com/product/gpt-4">GPT-4 (openai.com)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw">GPT-4震撼发布：多模态大模型，直接升级ChatGPT、必应，开放API，游戏终结了？ (qq.com)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/gpt-4.pdf">gpt-4.pdf (openai.com)</a></p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-14T12:03:26.000Z" title="2023/3/14 20:03:26">2023-03-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-15T08:03:51.233Z" title="2023/3/15 16:03:51">2023-03-15</time></span><span class="level-item">5 minutes read (About 690 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/14/2303142003/">Paper | Visual ChatGPT Talking, Drawing and Editing with Visual Foundation Models | arXiv2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</code></li>
<li>Keyword：Large Language Model（LLM），Visual Foundation Model（VFM）</li>
<li>Idea：<strong>Prompt Engineering</strong></li>
<li>Source<ul>
<li>Paper，2023年3月8日ArXiv Submitted，微软亚洲研究院的一项新工作。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.04671">2303.04671] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (arxiv.org)</a></li>
<li>Code，刚发布几天，目前已经有1万多标星了。<a target="_blank" rel="noopener" href="https://github.com/microsoft/visual-chatgpt">microsoft&#x2F;visual-chatgpt: VisualChatGPT (github.com)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>存在的问题：</p>
<ul>
<li><p><strong>大型语言模型</strong>如ChatGPT利用<strong>单一语言模态</strong>训练，因此处理视觉信息的能力非常有限。</p>
</li>
<li><p>相比较而言，<strong>视觉基础模型</strong>（VFM，Visual Foundation Models）在计算机视觉方面潜力巨大，因而能够理解和生成复杂的图像（如ViT、BLIP、Stable Diffusion等等）。VFM模型对输入-输出格式的苛求和固定限制，使得其在<strong>人机交互方面不如会话语言模型灵活</strong>。</p>
</li>
</ul>
<p>贡献：</p>
<ul>
<li>Prompt Engineering：将ChatGPT和多个SOTA视觉基础模型连接。</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>没有任何的训练，系统构成：</p>
<ul>
<li><p>Part 1 ChatGPT（直接利用大语言集成工具<strong>LangChain</strong>，调用OpenAI text-davinci-003 version）</p>
<p><img src="/2023/03/14/2303142003/1.png"></p>
</li>
<li><p>Part 2 PromptManager</p>
<p>构造了一个巨大的Prompt，把系统规则、视觉基础模型调用、历史对话、用户query、历史推理、中间结果都包含，简单来说就是<strong>指导ChatGPT怎么调用模型，什么时候调用，怎么处理结果</strong>。ChatGPT和VFMs之间沟通提到图片的时候使用的是随机生成的<strong>uuid（universally unique identifier）</strong>，两者之间是没有向量或者图片数据交互的。</p>
<p><img src="/2023/03/14/2303142003/2.png"></p>
<p><img src="/2023/03/14/2303142003/3.png"></p>
<p><img src="/2023/03/14/2303142003/4.png"></p>
</li>
<li><p>Part 3 VFMs（22个训练好的SOTA视觉基础模型，直接调用，利用4张V100就能全部部署）</p>
<p><img src="/2023/03/14/2303142003/5.png"></p>
</li>
</ul>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><ul>
<li>不是真正的多模态大模型，不过是普通玩家（小公司）可以尝试的Prompt Engineering。</li>
<li>训练一个多任务的large-scale视觉-语言模型应该非常消耗算力吧，23年3月15日发布的gpt4虽然没有公开详细的技术细节，但我觉得底层加了Vision QA，也就是Image-to-Text的能力，还是很难将I2I，T2I，I2T完全结合再一起的。</li>
<li>不过大力出奇迹，stack more layers，feed more data。</li>
<li>猜测GPT4背后的一些图像能力是靠这样的简单逻辑实现的。</li>
</ul>
<p><img src="/2023/03/14/2303142003/6.png"></p>
<p><img src="/2023/03/14/2303142003/7.png"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a target="_blank" rel="noopener" href="https://www.aminer.cn/research_report/640c27d07cb68b460fa0a702">视觉ChatGPT来了，微软发布，代码已开源 - 热点 - 科研解读 - AMiner</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613133999">visual-chatgpt: 训什么练，我直接prompt一把梭 - 知乎 (zhihu.com)</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-07T13:03:34.000Z" title="2023/3/7 21:03:34">2023-03-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-14T12:03:57.788Z" title="2023/3/14 20:03:57">2023-03-14</time></span><span class="level-item">a minute read (About 203 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/07/2303072103/">Frontend | Icarus主题美化</a></p><div class="content"><ol>
<li><p>为博客添加nest动态线条特效</p>
<p>在<code>themes\icarus\layout\layout.jsx</code>的<code>body</code>中添加如下代码，CDN可根据自己使用的修改。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span> color=<span class="string">&quot;30,144,255&quot;</span> opacity=<span class="string">&#x27;0.5&#x27;</span> zIndex=<span class="string">&quot;-1&quot;</span> count=<span class="string">&quot;150&quot;</span> src=<span class="string">&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>

<p>除了通过CDN加载，也可以下载到本地使用，详见<a target="_blank" rel="noopener" href="https://github.com/hustcc/canvas-nest.js">官方文档</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">color=&quot;255,255,255&quot; opacity=&#x27;0.7&#x27; # 改成了自己喜欢的颜色</span><br></pre></td></tr></table></figure>
</li>
<li><p>Code Highlight </p>
<p>可以从这些code highlight中找自己喜欢的styles。<a target="_blank" rel="noopener" href="https://github.com/highlightjs/highlight.js/tree/9.18.1/src/styles">highlight.js&#x2F;src&#x2F;styles at 9.18.1 · highlightjs&#x2F;highlight.js (github.com)</a></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article:</span></span><br><span class="line">    <span class="comment"># Code highlight settings</span></span><br><span class="line">    <span class="attr">highlight:</span></span><br><span class="line">        <span class="comment"># Code highlight themes</span></span><br><span class="line">        <span class="comment"># https://github.com/highlightjs/highlight.js/tree/master/src/styles</span></span><br><span class="line">        <span class="attr">theme:</span> <span class="string">xt256</span> <span class="comment"># 赛博朋克主题的暗黑code highlight</span></span><br><span class="line">        <span class="comment"># Show copy code button</span></span><br><span class="line">        <span class="attr">clipboard:</span> <span class="literal">true</span></span><br><span class="line">        <span class="comment"># Default folding status of the code blocks. Can be &quot;&quot;, &quot;folded&quot;, &quot;unfolded&quot;</span></span><br><span class="line">        <span class="attr">fold:</span> <span class="string">unfolded</span></span><br></pre></td></tr></table></figure>

<p>我的最爱：monokai</p>
</li>
</ol>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.anticme.com/2021/03/26/icarus%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE/">icarus个性化配置 - Hongjie’s blog (anticme.com)</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-05T12:40:39.000Z" title="2023/3/5 20:40:39">2023-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-05T12:46:39.163Z" title="2023/3/5 20:46:39">2023-03-05</time></span><span class="level-item">4 minutes read (About 558 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/05/2303052040/">Paper | ZITS++ Image Inpainting by Improving the Incremental Transformer on Structural Priors | arXiv2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors</code></li>
<li>Keyword：Transformer, High resolution Image Inpainting</li>
<li>Idea：之前CVPR2022会议文章的期刊版本，做了一些小改进和其他的尝试。</li>
<li>Source<ul>
<li>Paper，2022年10月第一版，2023年2月23日第二版（新鲜出炉的）。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.05950">2210.05950] ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors (arxiv.org)</a></li>
<li>Code，<a target="_blank" rel="noopener" href="https://github.com/DQiaole/ZITS_inpainting">DQiaole&#x2F;ZITS_inpainting: Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (CVPR2022) (github.com)</a>，<a target="_blank" rel="noopener" href="https://dqiaole.github.io/ZITS_inpainting/">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (dqiaole.github.io)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>ZITS存在的问题：</p>
<ul>
<li>ZITS中使用的canny边缘不能区分有意义的结构。在复杂环境中Canny边缘产生confusing textures而不是具有丰富信息的底层结构。</li>
</ul>
<p><img src="/2023/03/05/2303052040/1.png"></p>
<ul>
<li>深入研究不同的图像先验信息引导的高分辨率图像修复是必要的。</li>
<li>提升LaMa的纹理修复性能。</li>
</ul>
<p>贡献点：</p>
<ul>
<li>在原始的ZITS上（transformer-based的边缘和线框补充），又加入了许多不同先验的实验分析和讨论，最终发现L-Edges、线框和梯度先验结合效果最好。</li>
<li>将补全好的先验信息融合到修复网络中需要上采样，提出了一种Edge Non-Maximum Suppression（E-NMS），将冗余的边缘信息过滤掉（消除边界附近的模糊边缘）。</li>
<li>对于LaMa进行修改，加入了<strong>Large Kernel Attention</strong>以及修改模型设计。（增益：large receptive fields and scale invariance尺度不变性。we promote the <strong>maxpool as the mask resizing strategy</strong> of PatchGAN instead of the nearest in LaMa）</li>
</ul>
<p><img src="/2023/03/05/2303052040/2.png"></p>
<ul>
<li>提供了一个高分辨率图像数据集，HR-Flickr。</li>
</ul>
<p><img src="/2023/03/05/2303052040/3.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/05/2303052040/4.png"></p>
<ul>
<li>提出了learning-based边缘CATS取代原来用的canny边缘。并利用E-NMS（现有的算法）过滤不确定的边缘。最终使用的先验是CAT+线框（wireframe）+梯度。</li>
</ul>
<p><img src="/2023/03/05/2303052040/5.png"></p>
<ul>
<li><p>利用扩张卷积分解large Kernel，实验中取K&#x3D;21。</p>
</li>
<li><p>mask resize策略：maxpool取代nearest resize（稳定训练过程）</p>
</li>
</ul>
<p><img src="/2023/03/05/2303052040/6.png"></p>
<h1 id="Evalutaion"><a href="#Evalutaion" class="headerlink" title="Evalutaion"></a>Evalutaion</h1><ul>
<li>定量性能提升明显。</li>
</ul>
<p><img src="/2023/03/05/2303052040/7.png"></p>
<ul>
<li>定量效果也很好。</li>
</ul>
<p><img src="/2023/03/05/2303052040/8.png"></p>
<ul>
<li>人脸修复效果也很好。</li>
</ul>
<p><img src="/2023/03/05/2303052040/9.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-05T08:52:52.000Z" title="2023/3/5 16:52:52">2023-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-05T09:00:36.645Z" title="2023/3/5 17:00:36">2023-03-05</time></span><span class="level-item">13 minutes read (About 2019 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/05/2303051650/">Paper | Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding | CVPR2022</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</code></li>
<li>Keyword：Transformer, High resolution Image Inpainting</li>
<li>Idea：<strong>Extract edges and contours with Transformer</strong>, Masking Positional Encoding</li>
<li>Source<ul>
<li>Paper，2022年3月submitted的，到现在已经一年过去了，accepted in CVPR2022。[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.00867">2203.00867] Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (arxiv.org)</a></li>
<li>Code，基于LaMa做的一些小改进。<a target="_blank" rel="noopener" href="https://github.com/DQiaole/ZITS_inpainting">DQiaole&#x2F;ZITS_inpainting: Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (CVPR2022) (github.com)</a>，<a target="_blank" rel="noopener" href="https://dqiaole.github.io/ZITS_inpainting/">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (dqiaole.github.io)</a></li>
<li>PaperReading，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/496739824">CVPR2022|基于Transformer结构增强的增量式图像修复|ZITS - 知乎 (zhihu.com)</a>非常好的阅读笔记。</li>
</ul>
</li>
</ul>
<h1 id="Abstract-v1"><a href="#Abstract-v1" class="headerlink" title="Abstract v1"></a>Abstract v1</h1><p>本文是基于WACV’22的高分辨率图像修复工作LaMa进一步改进的，更偏向于<strong>自然场景的修复（更注重结构、轮廓的先验信息）</strong>。</p>
<p>现存的问题：</p>
<ul>
<li><p>1）现有的方法受限于CNN<strong>有限的感受野</strong>，只能处理常规的纹理，仍存在恢复生动纹理与合理的<strong>整体结构</strong>的问题（Vivid textures and Reasonable structures）。</p>
</li>
<li><p>2）Attention-based模型（Transformer）虽然能更好的学习长距离依赖（Long-range dependency），但是受限于高分辨率图像推理时的<strong>Heavy Computation</strong>。</p>
</li>
</ul>
<p>解决的方法（贡献）：</p>
<ul>
<li>1）【主要贡献】An additional structure restorer，增加一个额外的结构修复器，增量式的辅助图像修复。<ul>
<li>在固定的低分辨率Sketch space（Gray-scale space）修复整体的结构，并可以通过上采样融入到修复过程中。</li>
<li>Can be integrated with other pretrained inpainting models efficiently with the zero-initialized residual addition（无需额外训练，直接融入到其他Inpainting预训练模型中）。</li>
</ul>
</li>
<li>2）Masking positional encoding strategy用于提升使用Large irregular mask训练的性能。</li>
</ul>
<h1 id="Abstract-v2"><a href="#Abstract-v2" class="headerlink" title="Abstract v2"></a>Abstract v2</h1><p>现存的问题：</p>
<ul>
<li>现有的Inpainting方法只能处理regular textures，由于CNN感受野有限的问题，失去了对于图像<strong>整体结构（Holistic Structure）</strong>的把控。</li>
<li>基于attention的方法可以一定程度上解决该问题，但受限于高分辨率图像推理时的<strong>Heavy Computation</strong>。</li>
</ul>
<p>贡献：</p>
<ul>
<li>Motivation：对于高分辨率自然图像修复来说，边缘信息十分重要，如果没有对于大图像的整体理解，很难恢复场景的边缘和线条，尤其是纹理较弱的场景。Method：使用一个<strong>额外的结构恢复网络</strong>，增量式的辅助图像修复过程。具体而言：transformer-based网络，在固定的低分辨率草图空间中，修复图像的<strong>边缘和轮廓线条</strong>，而后上采样到高分辨率，融合到后续图像修复网络中。</li>
<li><strong>Zero-initialized Residual Addition</strong>（零初始化残差融合）增量训练策略：提出的方法可以和其他的pretrained inpainting model轻易的整合在一起（许多其他利用先验信息的方法通常是多阶段多模型，训练成本高，而这个策略可以在较少的step数中快速收敛）。</li>
<li>提出了一个<strong>Masking Positional Encoding Strategy</strong>，提升在大mask配置下的模型性能。（高分辨率、较大缺失区域的修复，模型前期会在mask区域重复产生没有语义的伪影，浪费计算量）</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>Image Inpainting Goal：The inpainted images should remain both <strong>semantically coherent textures</strong> and <strong>visually reasonable structures</strong>. 这里也给了我们一点点启发，对于人脸修复而言，语义一致性至关重要，所以利用语义分割信息来引导人脸修复是一个好的想法；而后者，整体结构的连贯性，则对于自然场景图像修复至关重要。</li>
<li>Image Inpainting任务现存的问题<ul>
<li>1）Limited receptive fields。面对large corrupted region和高分辨率图像时问题更加凸显。</li>
<li>2）Missing holistic structures。缺乏整体结构，Recovering key edges and lines for scenes。</li>
<li>3） Heavy computations。训练高分辨率图像的GAN非常tricky and costly。</li>
<li>4） No positional information in masked regions。在大mask配置下，模型会生成没有意义伪影，浪费计算量。</li>
</ul>
</li>
</ul>
<blockquote>
<p>很好，我的另一个Idea别人也已经实现了，好好看好好学吧(●’◡’●)</p>
</blockquote>
<ul>
<li>作者分析了LaMa的不足之处（其实非常明显），LaMa的本质是在频域内做了1×1卷积保证了相同周期性信号的关联，也就是LaMa作者想要解决的重复性纹理的修复。但是这样的方法<strong>无法确保整体结构</strong>，并且在<strong>纹理较弱的图像</strong>上性能很差。</li>
</ul>
<blockquote>
<p>最先使用transformer-based做low-resolution图像修复，然后再CNN上采样超分一下的工作。</p>
<ul>
<li><p>Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. High-fidelity pluralistic image completion with transformers. arXiv preprint arXiv:2103.14031, 2021.</p>
</li>
<li><p>Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying Ma, Xuansong Xie, and Chunyan Miao. Diverse image inpainting with bidirectional and autoregressive transformers. arXiv preprint arXiv:2104.12335, 2021.</p>
</li>
</ul>
<p>还有许多使用先验信息的网络，但通常都是多阶段图像修复，训练成本较高（trained from scratch）。</p>
</blockquote>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/05/2303051650/1.png"></p>
<ul>
<li>首先将mask、masked image（valid pixel为1，待填充区域为0，mask可视化时反转一下，待填充变为1，都是为了方便后续计算）、canny边缘提取器获得的masked edge（边缘）以及利用作者之前提出的模型获取的masked lines（线框，主要是建模两点之间的连线，所以上采样下采样时不存在歧义，但是canny边缘提取出来的信息在不同feature size提取出的边缘可能不同）。</li>
<li>送入TSR，首先将256×256的图片下采样三次到32×32大小，然后利用基于轴向注意力和常规注意力的transformer，减少计算量提升计算效率，最后获得256×256的修复后的边缘和线框。后续利用一个简单的四层CNN网络来对于修复好的先验信息进行上采样，只用线框数据进行训练而不用线框加边缘数据，这样做能够更好的消除歧义，获得不同分辨率更加一致的先验信息。</li>
<li>因为边缘和线框信息是稀疏的，所以利用基于门控卷积的网络来提取更关键的信息，并采用多尺度信息，也就是中间block的最后一层和上采样的三层，通过零初始化残差融合（就是做了一个简单的残差运算），和baseline LaMa的前四层融合在一起，然后训练50k进行一个增量学习微调就能显著的提升原模型的效果。</li>
<li>至于MPE（Masking Positional Encoding），其实就是取一个3×3的all-one卷积核来和mask区域做计算，能够获得距离大mask中心的距离信息以及mask方向信息，送入到baseline网络中作为辅助信息。（黑色区域为1白色为0，很简单的卷积运算）。</li>
</ul>
<p><img src="/2023/03/05/2303051650/2.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>主要针对自然场景图像修复，定性上的性能增益不是很明显。</li>
</ul>
<p><img src="/2023/03/05/2303051650/3.png"></p>
<p><img src="/2023/03/05/2303051650/4.png"></p>
<ul>
<li>MPE这个方法更是鸡肋，出发点很好但是做的太简单了，所以也没有多高的性能增益。</li>
</ul>
<p><img src="/2023/03/05/2303051650/5.png"></p>
<ul>
<li>但是定性效果很好，主要是整体结构信息（边缘和线框）对于高分辨率的自然场景图像来说是非常关键的信息。作者之前提出的提取线框的模型，我觉得底层逻辑就像是透视图，对于空间布局来说，透视图很重要，所以修复出来的图片效果会更好。</li>
</ul>
<p><img src="/2023/03/05/2303051650/6.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-28T11:21:54.000Z" title="2023/2/28 19:21:54">2023-02-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-02-28T11:43:28.023Z" title="2023/2/28 19:43:28">2023-02-28</time></span><span class="level-item">5 minutes read (About 732 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/28/2302281930/">Issues | Baseline MISF-CVPR2022 Reprod &amp; GIQA improve</a></p><div class="content"><ul>
<li>Official code：<a target="_blank" rel="noopener" href="https://github.com/tsingqguo/misf">tsingqguo&#x2F;misf (github.com)</a></li>
</ul>
<h1 id="关于loss参数"><a href="#关于loss参数" class="headerlink" title="关于loss参数"></a>关于loss参数</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;epoch&#x27;, 1), (&#x27;iter&#x27;, 1), (&#x27;l_d2&#x27;, 0.707538366317749), (&#x27;l_g2&#x27;, 0.07427514344453812), (&#x27;l_l1&#x27;, 0.7772688865661621), (&#x27;l_per&#x27;, 0.20167401432991028), (&#x27;l_sty&#x27;, 0.393798291683197)]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logs = [</span><br><span class="line">            (<span class="string">&quot;l_d2&quot;</span>, dis_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_g2&quot;</span>, gen_gan_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_l1&quot;</span>, gen_l1_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_per&quot;</span>, gen_content_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_sty&quot;</span>, gen_style_loss.item()),</span><br><span class="line">        ]</span><br></pre></td></tr></table></figure>

<p>其中，l_d2是Inpainting Model的Discriminator loss，l_g2是Inpainting Model的Generator loss，l_l1是L1 loss，l_per是Perceptual loss，l_sty是Style loss。</p>
<p>这篇文章作者的code是基于Edge Connect的代码Repo的，原模型Edge Connect分为了Edge model、Inpainting Model、Inpaint with Edge Model以及Joint Model四个训练阶段，这里MISF的作者应该是只用了Inainting Model的部分并进行了修改。</p>
<h1 id="wandb使用"><a href="#wandb使用" class="headerlink" title="wandb使用"></a>wandb使用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb  <span class="comment"># 使wandb库在pytorch库之后引用</span></span><br><span class="line"></span><br><span class="line">default_config = <span class="built_in">dict</span>(</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    dropout=<span class="number">0.5</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">wandb.init(project=<span class="string">&quot;pj-name&quot;</span>, config=default_config, mode=<span class="string">&quot;online/offline/disabled&quot;</span>)</span><br><span class="line"></span><br><span class="line">batch_size = wandb.config.batch_size <span class="comment"># 保证代码可读性和一致性</span></span><br><span class="line"></span><br><span class="line">wandb.log(&#123;<span class="string">&#x27;epoch&#x27;</span>: epoch, <span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;accuracy&#x27;</span>: accuracy&#125;)</span><br></pre></td></tr></table></figure>

<h1 id="Package-import"><a href="#Package-import" class="headerlink" title="Package import"></a>Package import</h1><p><code>sys.path</code>指定模块搜索路径的<strong>列表</strong>。默认情况下，<code>python</code>导入文件或者模块，会在<code>sys.path</code>里找模块的路径。如果路径下搜索不到模块的话，就会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="built_in">print</span>(sys.path)</span><br><span class="line">sys.path.append(<span class="string">&#x27;/home/nsy/nlp&#x27;</span>) <span class="comment"># package路径为/home/nsy/nlp/new_package</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;/home/user5/code/misf-main&#x27;, &#x27;/home/user5/.pycharm_helpers/pydev&#x27;, &#x27;/home/user5/code/misf-main&#x27;, &#x27;/home/user5/.pycharm_helpers/pycharm_display&#x27;, &#x27;/home/user5/.pycharm_helpers/third_party/thriftpy&#x27;, &#x27;/home/user5/.pycharm_helpers/pydev&#x27;, &#x27;/home/user5/code/misf-main/C&#x27;, &#x27;/Users/75796/AppData/Local/JetBrains/PyCharm2021.3/cythonExtensions&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python38.zip&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8/lib-dynload&#x27;, &#x27;/home/user5/.local/lib/python3.8/site-packages&#x27;, &#x27;/home/user5/code/PUT-main&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8/site-packages&#x27;, &#x27;/home/user5/.pycharm_helpers/pycharm_matplotlib_backend&#x27;]</span><br></pre></td></tr></table></figure>

<h1 id="后台训练"><a href="#后台训练" class="headerlink" title="后台训练"></a>后台训练</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u main.py &gt;02272115_loss.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h1 id="GIQA升级版FIQA"><a href="#GIQA升级版FIQA" class="headerlink" title="GIQA升级版FIQA"></a>GIQA升级版FIQA</h1><ul>
<li>Best model？</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- checkpoints/acc01090300/model_best.pth.tar</span><br><span class="line">- /home/user5/code/QA/GIQA-master/MBC-GIQA/checkpoints/acc01090300/model_best.pth.tar</span><br></pre></td></tr></table></figure>

<ul>
<li>Freeze pretrained layer</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/311095447">(29 封私信 &#x2F; 7 条消息) Pytorch 如何精确的冻结我想冻结的预训练模型的某一层，有什么命令吗？ - 知乎 (zhihu.com)</a></p>
<h1 id="算法工程师升级打怪"><a href="#算法工程师升级打怪" class="headerlink" title="算法工程师升级打怪"></a>算法工程师升级打怪</h1><ul>
<li><p>成为一个算法工程师首先你得有工程能力，就是说你得先能干活，<strong>熟练的掌握一门编程语言</strong>必不可少；</p>
</li>
<li><p>然后是<strong>相关领域的专业知识</strong>，比如推荐算法，你需要了解常见推荐算法的原理、优缺点、应用场景等；</p>
</li>
<li><p>然后是<strong>机器学习的基础知识</strong>，李航的《统计机器学习》，周志华的《机器学习》，Benjio的《深度学习》，这三本书至少得过个那么一两遍吧，把基础知识掌握牢了再学习其它的就容易多了，基础不牢地动山摇；</p>
</li>
<li><p>然后是掌握一些<strong>数据结构和算法知识</strong>，这个还是比较重要的，对你写出高效的代码很有帮助。</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-26T14:47:11.000Z" title="2023/2/26 22:47:11">2023-02-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-02T06:32:05.730Z" title="2023/3/2 14:32:05">2023-03-02</time></span><span class="level-item">8 minutes read (About 1234 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/26/2302262250/">Paper | Resolution-robust Large Mask Inpainting with Fourier Convolutions | WACV2022</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Resolution-robust Large Mask Inpainting with Fourier Convolutions</code></li>
<li>Keyword：Large Mask Inapinting</li>
<li>Idea：Fourier Convolutions</li>
<li>Source<ul>
<li>Paper，2021年9月15日submitted的。最后发表在WACV2022上，确实是Applications of CV，非常实用。后续有很多CVPR2022的高分辨率图像修复任务都和这篇工作做了对比。[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.07161">2109.07161] Resolution-robust Large Mask Inpainting with Fourier Convolutions (arxiv.org)</a></li>
<li>Code，大分辨率图像修复效果非常好的一项工作，面向落地的。<a target="_blank" rel="noopener" href="https://github.com/saic-mdal/lama%EF%BC%8C[Resolution-robust">https://github.com/saic-mdal/lama，[Resolution-robust</a> Large Mask Inpainting with Fourier Convolutions (advimman.github.io)](<a target="_blank" rel="noopener" href="https://advimman.github.io/lama-project/">https://advimman.github.io/lama-project/</a>)</li>
<li>Vedio，超棒的一个paper讲解，非作者本人，但是邀请了一作来interview。<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Lg97gWXsiQ4">Resolution-robust Large Mask Inpainting with Fourier Convolutions (w&#x2F; Author Interview) - YouTube</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现存的问题：</p>
<ul>
<li>Modern image inpainting systems, often struggle with <strong>large missing areas</strong>, <strong>complex geometric structures</strong>, and <strong>high-resolution images</strong>. 目前图像修复存在的问题有：大缺失区域（但个人认为ill-posed problem不是傅里叶卷积能够解决的）、复杂几何结构以及高分辨率图像修复。</li>
</ul>
<p>猜想：</p>
<ul>
<li>如何解决这个问题？作者认为最主要的原因是lack of an <strong>effective receptive field</strong> in both the inpainting <strong>network and the loss function</strong>.</li>
</ul>
<p>本文LaMa（Large mask inpainting）贡献点：</p>
<ul>
<li>在网络结构上，使用fast Fourier convolutions的inpainting network architecture，image-wide的感受野（快速傅里叶卷积的贡献）。</li>
<li>在损失函数上，A high receptive field perceptual loss。</li>
<li>在训练策略上，使用Large training mask。</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>A large effective receptive field</strong> is essential for understanding the global structure of an image.</p>
<ul>
<li>第一， <strong>high receptive field architecture</strong>。文章提出了基于快速傅里叶卷积（FFCs）的网络架构，能够使得网络前几层感受野都能cover整个图像。可以提升perceptual quality并使网络轻量化，而且泛化能力很强（即使训练集不包含的高分辨率图像，也能很好的推理）。</li>
<li>第二， <strong>high receptive field loss function</strong>。文章提出基于语义分割网络、大感受野的perceptual loss。能够提升全局结构和形状的一致性。</li>
<li>第三，<strong>aggressive algorithm of training masks generation</strong>。training mask generation，生成更大的mask。</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>大mask配置下，如果依旧利用传统的3×3ResNet卷积核，在网络前期感受野可能位于掩膜内部，所以网络中的许多层都缺乏全局上下文，浪费了计算量和参数。</p>
<h2 id="Add-FFC"><a href="#Add-FFC" class="headerlink" title="Add FFC"></a>Add FFC</h2><ul>
<li><p>而Fast Fourier convolution (FFC) 能够让网络前几层应用全局的上下文信息。包含两个并行分支，1）局部分支使用常规的卷积操作；2）全局分支使用real FFT，作用在实数信号上。FFT会转换到复数空间（频域）。而inverse real FFT能够保证输出是实数。</p>
</li>
<li><p>这里简单的real FFT得到的复数实部和虚部concat到了一起，然后在频域上做了一个1×1卷积，也就是同频分量的卷积，这样能保证周期性信号的修复（也就是重复性的pattern，作者最初的motivation就是认为现有的方法对于重复性pattern修复的结果不佳，想到重复pattern就想到了周期性信号，也就使用了FFT来解决这个问题）</p>
</li>
</ul>
<p><img src="/2023/02/26/2302262250/1.png"></p>
<p><img src="/2023/02/26/2302262250/2.png"></p>
<p><img src="/2023/02/26/2302262250/3.png"></p>
<ul>
<li>提出了一个Fast Fourier Conv Residual Block，也就是res block改成快速傅里叶卷积。FFC还有局部分支和全局分支的交互，作用在每一个层之间。</li>
</ul>
<p><img src="/2023/02/26/2302262250/4.png"></p>
<h2 id="Perceptual-loss-pro"><a href="#Perceptual-loss-pro" class="headerlink" title="Perceptual loss pro"></a>Perceptual loss pro</h2><ul>
<li>在鉴别器部分，使用segmentation model作为backbone来专注于high-level information，而不是classification model backbone，更专注于纹理等低级特征。使用傅里叶或扩张卷积来实现均可。</li>
</ul>
<p><img src="/2023/02/26/2302262250/5.png"></p>
<ul>
<li>这里做了消融实验验证了对于perceptual loss升级后的效果。因为生成器更关注于全局信息，所以也要使判别器的性能提升，这样在GAN的训练过程中才能保持平衡。</li>
</ul>
<p><img src="/2023/02/26/2302262250/6.png"></p>
<h2 id="Generation-of-large-mask"><a href="#Generation-of-large-mask" class="headerlink" title="Generation of large mask"></a>Generation of large mask</h2><ul>
<li>输入的数据对于模型的性能提升很重要。与deepfillv2和narrow mask相比，文章生成large wide mask（多边形宽笔划）和large box mask的组合，作为训练输入。</li>
</ul>
<p><img src="/2023/02/26/2302262250/7.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>红色代表本方法比其他方法性能提升的百分比。可以看出在narrow mask配置下，本文方法超过绝大多数method，但是在wide mask配置下，性能吊打其他方法。</li>
</ul>
<p><img src="/2023/02/26/2302262250/8.png"></p>
<ul>
<li>使用傅里叶卷积的消融实验，在narrow mask下傅里叶卷积模型的性能提升效果不是很明显，但是大mask配置下优势就很突出。</li>
</ul>
<p><img src="/2023/02/26/2302262250/10.png"></p>
<ul>
<li>还可以泛化到高分辨率图像上。</li>
</ul>
<p><img src="/2023/02/26/2302262250/9.png"></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar1.png" alt="Vanessa Ni🛸🏴‍☠️"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Vanessa Ni🛸🏴‍☠️</p><p class="is-size-6 is-block">💗I am currently a first-year postgraduate student at Fudan University. My research focuses on computer vision👁️, especially image/face inpainting and image quality assessment. 💻I graduated from Tianjin University with a Bachelor&#039;s degree in Software Engineering, and acquired a little knowledge about 2d/3d object detection. I worked as an algorithm engineer intern at CATRC and Baidu. 📙I enjoy reading (especially psychology and philosophy), traveling🧳, photography📸, working out💪, and dancing💃 in my free time. 🚀I want to become an algorithm engineer or go to the U.S. for Ph.D. 3 years later.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">18</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Growth/"><span class="level-start"><span class="level-item">Growth</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Util/"><span class="level-start"><span class="level-item">Util</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-20T11:28:02.000Z">2023-03-20</time></p><p class="title"><a href="/2023/03/20/2303201927/">Paper | CoordFill, Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying | AAAI2023</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-20T11:17:32.000Z">2023-03-20</time></p><p class="title"><a href="/2023/03/20/2303201917/">Paper | Spatially-Adaptive Pixelwise Networks for Fast Image Translation | CVPR2021</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-17T06:05:11.000Z">2023-03-17</time></p><p class="title"><a href="/2023/03/17/2303171405/">Growth | Be a Techno-Optimist</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-15T08:51:48.000Z">2023-03-15</time></p><p class="title"><a href="/2023/03/15/2303151651/">Trend | GPT-4 —— The winner takes all</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-14T12:03:26.000Z">2023-03-14</time></p><p class="title"><a href="/2023/03/14/2303142003/">Paper | Visual ChatGPT Talking, Drawing and Editing with Visual Foundation Models | arXiv2023</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Growth/"><span class="tag">Growth</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/paper/"><span class="tag">paper</span><span class="tag">7</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Just be here now🎇</a><p class="is-size-7"><span>&copy; 2023 Vanessa Ni</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>