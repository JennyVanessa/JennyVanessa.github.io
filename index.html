<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Vanessa&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Vanessa&#039;s blog🎀"><meta name="msapplication-TileImage" content="/img/my_favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Vanessa&#039;s blog🎀"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Vanessa&#039;s blog"><meta property="og:url" content="https://jennyvanessa.github.io/"><meta property="og:site_name" content="Vanessa&#039;s blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jennyvanessa.github.io/img/og_image.png"><meta property="article:author" content="Vanessa Ni"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://jennyvanessa.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jennyvanessa.github.io"},"headline":"Vanessa's blog","image":["https://jennyvanessa.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Vanessa Ni"},"publisher":{"@type":"Organization","name":"Vanessa's blog","logo":{"@type":"ImageObject","url":{"text":"Just be here now🎇"}}},"description":""}</script><link rel="icon" href="/img/my_favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><script type="text/javascript" color="255,255,255" opacity="0.7" zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Just be here now🎇</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JennyVanessa"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-04T08:16:52.000Z" title="2023/4/4 16:16:52">2023-04-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-04-04T08:21:52.765Z" title="2023/4/4 16:21:52">2023-04-04</time></span><span class="level-item">6 minutes read (About 869 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/04/2304041616/">Paper | Perceptual Artifacts Localization for Inpainting | ECCV2022</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>Perceptual Artifacts Localization for Inpainting</code></p>
</li>
<li><p>Keyword：Artifacts Localization（瑕疵定位）</p>
</li>
<li><p>Idea：利用现有的语义分割模型对generated images进行二元分割，是首次检测瑕疵区域的方法，而不是检测hole area。</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2022年8月5号arXiv submitted，ECCV2022 Oral。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.03357">Perceptual Artifacts Localization for Inpainting (arxiv.org)</a></li>
<li>Code，<a target="_blank" rel="noopener" href="https://github.com/owenzlz/PAL4Inpaint">GitHub - owenzlz&#x2F;PAL4Inpaint: Perceptual Artifacts Localization for Inpainting, ECCV 2022 (Oral)</a>。</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现存的问题：</p>
<ul>
<li><p>图像修复任务是不适定的（ill-posed），在进行large holes修补时会生成<strong>瑕疵和伪影（Artifacts）</strong>。</p>
</li>
<li><p>在真实用户应用中，图像修复被用于<strong>前景物体去除</strong>（现有的irregular mask不如实例分割mask有效），用户更偏好于视觉上<strong>可信的背景生成</strong>（现有的有参考指标进行修复评估不合理），并<strong>迭代进行修复</strong>（缺少一个自动化定位瑕疵的方法）。</p>
</li>
</ul>
<p><img src="/2023/04/04/2304041616/1.png"></p>
<blockquote>
<p>迭代Inpainting方法</p>
<ul>
<li>Su S, Yan Q, Zhu Y, et al. Blindly assess image quality in the wild guided by a self-adaptive hyper network[C]&#x2F;&#x2F;Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition. 2020: 3667-3676.</li>
<li>Zeng Y, Lin Z, Yang J, et al. High-resolution image inpainting with iterative confidence feedback and guided upsampling[C]&#x2F;&#x2F;Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIX 16. Springer International Publishing, 2020: 1-17.</li>
</ul>
</blockquote>
<p>贡献：</p>
<p>提出<strong>新任务</strong>：<strong>Inpainting artifacts segmentation</strong>，构建数据集用于模型评估以及迭代细化。</p>
<ul>
<li><strong>Dataset</strong>：针对目前的SOTA Inpainting模型的生成的有perceptual artifacts的结果进行手工标注，以进行后续的语义分割，定位感知伪影区域。（4795张图片）</li>
<li><strong>Metric</strong>：提出一个叫做Perceptual Artifact Ratio（PAR）的评估指标，度量artifacts区域占整个Inpainted区域的比例。</li>
<li><strong>Iterative refinement</strong>：将生成的语义分割图应用在现有的SOTA模型上，进行迭代优化。</li>
</ul>
<p>由于商业原因，只开源一部分数据集和推理代码，训练代码和完整数据集不开源。大概有2000张训练图片、400张测试图片、400张验证图片。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><ul>
<li>使用ProFill、CoMod-GAN、LaMa生成用于标记的图像。在hole area周围放置膨胀的包围框，方便用户定位。聘请专业人员进行两轮检查，避免“漏标记”或“过度标记”区域。在4795张图中有832张没有瑕疵，加入训练避免预测假阳性。最终发现，感知伪影区域占孔掩膜区域的平均比例为29.67%。</li>
</ul>
<p><img src="/2023/04/04/2304041616/2.png"></p>
<p><img src="/2023/04/04/2304041616/3.png"></p>
<ul>
<li>在现有的backbone和head模型上进行训练，以及训练策略的消融。</li>
</ul>
<p><img src="/2023/04/04/2304041616/4.png"></p>
<p><img src="/2023/04/04/2304041616/5.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>不同方法生成的图片进行配对，然后进行2选1，看和人类观察者的选择一致的百分比。</li>
</ul>
<p><img src="/2023/04/04/2304041616/6.png"></p>
<ul>
<li>发现随着hole大小增加，感知瑕疵比例也在上升。</li>
</ul>
<p><img src="/2023/04/04/2304041616/7.png"></p>
<ul>
<li>进行迭代修复策略，随着迭代数的不断增加，瑕疵比例在不断下降。</li>
</ul>
<p><img src="/2023/04/04/2304041616/8.png"></p>
<p><img src="/2023/04/04/2304041616/9.png"></p>
<ul>
<li>但是感觉迭代修复后的图片只是更平滑了（大多数情况下），并没有完全解决瑕疵的问题，所以用户才会觉得大多数情况下保持一致，不过这里想要证明能够在30%的左右的情况下提升Inpainting效果，没有显著降低原始Inpainting模型性能。</li>
</ul>
<p><img src="/2023/04/04/2304041616/10.png"></p>
<p><img src="/2023/04/04/2304041616/11.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-04T08:11:21.000Z" title="2023/4/4 16:11:21">2023-04-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-04-04T08:16:24.788Z" title="2023/4/4 16:16:24">2023-04-04</time></span><span class="level-item">3 minutes read (About 498 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/04/2304041611/">Paper | High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling | ECCV2020</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling</code></p>
</li>
<li><p>Keyword：Iterative Inpainting</p>
</li>
<li><p>Idea：利用一个解码器分支预测置信度图，设定阈值，大于0.5的生成结果保存，小于0.5的修复结果舍弃，并进行下一次迭代修复。</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2020年5月24号arXiv submitted，ECCV2020。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.11742">High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling (arxiv.org)</a></li>
<li>Code，只开源了API接口。<a target="_blank" rel="noopener" href="https://zengyu.me/iic/">iic (zengyu.me)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>以往的图像修复方法通常使用square-shaped or irregular holes，然而在实际场景中，用户想要去除object或scene实例片段（segment）。</li>
</ul>
<p><img src="/2023/04/04/2304041611/1.png"></p>
<ul>
<li>当时的SOTA方法在真实的Object removal request下效果不佳。</li>
</ul>
<p><img src="/2023/04/04/2304041611/2.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/04/04/2304041611/3.png"></p>
<ul>
<li><p>在数据集上，从语义分割、视频分割等数据集收集了82020object mask，并利用random strokes（free-formed mask）和构建的object mask一同训练，避免对于object shaped holes的bias。</p>
</li>
<li><p>网络使用了course-to-fined架构，精细优化网络有一个编码器和两个解码器，一个生成修复结果，另一个返回预测像素值的置信度图。</p>
</li>
</ul>
<p><img src="/2023/04/04/2304041611/4.png"></p>
<ul>
<li><p>前一项就是对掩膜区域内置信的pixel计算loss，后一项正则来保证置信pixel的数量（其中C代表置信的pixel的集合，如果没有正则，网络偏向于将所有pixel置信度设为0）。设置置信度阈值0.5，低于置信阈值的复原部分不去补全它（不置信），高于阈值的复原部分才补全回去，补全回去后就进行下一次迭代。</p>
</li>
<li><p>参考小尺度补全的patch相似性来进行高分辨率补全。高分辨率的patch feature map由在低分辨率patch计算的相似度来引导。</p>
<p><img src="/2023/04/04/2304041611/5.png"></p>
</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p><img src="/2023/04/04/2304041611/6.png"></p>
<ul>
<li>在不同缺失区域比例下，超过了当时的SOTA。</li>
</ul>
<p><img src="/2023/04/04/2304041611/7.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-27T09:23:48.000Z" title="2023/3/27 17:23:48">2023-03-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-27T09:27:43.279Z" title="2023/3/27 17:27:43">2023-03-27</time></span><span class="level-item">5 minutes read (About 695 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/27/2303271723/">Paper | SmartBrush, Text and Shape Guided Object Inpainting with Diffusion Model | CVPR2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model</code></p>
</li>
<li><p>Keyword：<strong>Diffusion Model</strong> for Inpainting</p>
</li>
<li><p>Idea：利用实例分割mask和文本标签（实例分类或BLIP生成的长句描述）进行精细的Object Inpainting。</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2022年12月9号arXiv submitted，CVPR2023。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.05034">SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model (arxiv.org)</a></li>
<li>Code，未开源。</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><img src="/2023/03/27/2303271723/1.png"></p>
<p>现存的Diffusion-based Inpainting Model存在如下问题：</p>
<ul>
<li><strong>Text misalignment</strong>：由于训练过程中的text prompt是针对全局图片的，所以模型会将mask region中填补背景内容，而不是根据针对mask region的text prompt进行精细的修复（e.g. Glide、Stable Inpainting，会把火烈鸟生成在背景部分）。</li>
<li><strong>Mask misalignment</strong>：结合多模态language-vision模型会捕捉全局信息，不会在给定的mask形状内生成内容（Blended diffusion）。</li>
<li><strong>Background preservation</strong>：会不经意修改inpainting object周围的背景区域（e.g. bottom row）。</li>
</ul>
<p>贡献：</p>
<ul>
<li>对于mask的精度进行控制，设定一个<strong>precision factor</strong>超参数，使模型对于不同精度的mask敏感。</li>
<li>训练时使用<strong>实例分割mask</strong>以及对应的局部文本描述。</li>
<li><strong>进行实例mask预测</strong>，并作为正则化损失，使模型只在实例mask区域内生成内容。</li>
<li>多任务训练，联合训练inpainting任务和text-to-image generation，在pretrained模型上微调，以利用更多训练数据。</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/27/2303271723/2.png"></p>
<p>在原有的diffusion模型的基础上</p>
<ul>
<li>只对mask区域加噪，学习上下文信息</li>
</ul>
<p><img src="/2023/03/27/2303271723/3.png"></p>
<ul>
<li>针对不同精度的mask，加入不同的条件控制，其中s是mask精度，m_s是实例分割对应的mask，c是实例的text label。</li>
</ul>
<p><img src="/2023/03/27/2303271723/4.png"></p>
<ul>
<li>不论是什么样精度的mask，都要预测最精细的实例mask。</li>
</ul>
<p><img src="/2023/03/27/2303271723/5.png"></p>
<p>在已有的text-to-image生成模型（stable diffusion、Imagen）的预训练模型上微调，训练过程中将text-to-image的input mask设置为整张图片大小，作为inpainting的一个特殊情况。在8卡A100上训练了20K steps。</p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>个人认为这种限定实例分割mask区域内修复的物体的效果，不如全局上直接生成效果好（更能fit in进原图，并且再画风上能保持一致性，因为是全局嵌入）</li>
<li>就像后期的一些diffusion-based+prompt2prompt方法进行图像编辑一样，或许人类指令（文本）会比精细mask的效果更好（实际上都是粗mask应用）、且交互性更强。</li>
<li>人类指令+粗mask或许是个值得follow的idea。</li>
</ul>
<p><img src="/2023/03/27/2303271723/6.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-25T14:16:24.000Z" title="2023/3/25 22:16:24">2023-03-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-26T16:00:40.408Z" title="2023/3/27 00:00:40">2023-03-27</time></span><span class="level-item">6 minutes read (About 841 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/25/2303252216/">Paper | Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning | AAAI2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning</code></p>
</li>
<li><p>Keyword：Contrastive Learning、Segmentation Confusion Adversarial Training</p>
</li>
<li><p>Idea：Using Semantaion network predicts mask to distinguish generated and valid region，optimizing in an adversarial-style（between inpainting generator and a segmentation network）</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2023年3月23号arXiv submitted，AAAI2023 Oral。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.13133">Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning (arxiv.org)</a></li>
<li>Code，未开源。</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>【提出了一个针对Image Inpainting任务的对抗训练framework】（很勉强的说法），利用所提出的segmentation confusion adversarial training (SCAT) and contrastive learning。</li>
</ul>
<p><img src="/2023/03/25/2303252216/1.png"></p>
<ul>
<li><p>结合SCAT和全局对抗训练，有以下三个优势。</p>
<ul>
<li>repaired image的全局一致性</li>
<li>提升repaired image的局部精细纹理细节</li>
<li>处理不同图片free-form holes灵活性</li>
</ul>
<p>这是现存的对抗训练框架难以同时满足的（但这对比的“现存”方法也都太考古了）。</p>
</li>
</ul>
<p><img src="/2023/03/25/2303252216/2.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>Motivation是人类能够轻易的寻找出扭曲或者纹理不一致的区域。本文利用分割网络来模拟这种人类行为，标记输入图像中的generated region和valid region，本质上是一个二分类语义分割。修复网络来“欺骗”分割网络，生成视觉语义上更合理填充内容，使得分割网络难以区分标记生成的部分（generated region）和原始的像素（valid region）。</p>
<p><img src="/2023/03/25/2303252216/3.png"></p>
<ul>
<li>其中为m为输入mask，m-bar是全1mask代表真实图片的mask，repaired image为x-bar，gt image为x。对于inpainting generator G而言，利用m-bar来作为repaired image x-bar的监督，使得生成器能够生成更好的修复图像，以混淆分割网络S。S部分就是m作为repaired image的监督，m-bar作为gt的，使得S作为鉴别器学习不同图片内容中generated和valid像素分布。</li>
</ul>
<p><img src="/2023/03/25/2303252216/4.png"></p>
<p>为了稳定GAN训练，加入对比学习损失，通过利用鉴别器的特征表示空间，在这个空间中将generated image分布拉向ground truth分布，同时远离corrupted image的分布。【作者说这是第一次将对比学习融入图像修复，说的很勉强】</p>
<ul>
<li>x-tilde为受损图片（就是mask和gt点乘得到的），鉴别器low-level特征为局部纹理细节，high-level为全局语义信息。取low-levelN个特征层计算纹理对比损失，接近gt分布而远离x-tilde（受损图像）分布；取鉴别器最后一个输出特征层计算语义对比损失，并构建多个x-tilde作为不同受损版本图片（超参数取8）。</li>
</ul>
<p><img src="/2023/03/25/2303252216/5.png"></p>
<p><img src="/2023/03/25/2303252216/6.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>对比的新方法WaveFill、LaMa都是基于频率的，CTSDG是基于边缘先验的，在纹理复杂情况下效果都会差。</li>
</ul>
<p><img src="/2023/03/25/2303252216/7.png"></p>
<ul>
<li>在CelebA数据集上的人脸修复定性比烂结果。</li>
</ul>
<p><img src="/2023/03/25/2303252216/8.png"></p>
<ul>
<li>不过在irregular mask+CelebA数据集上的定性数据挺好看的，但没做过这个数据集以及places2数据集上的实验，不能确定这个效果是否性能很好。</li>
</ul>
<p><img src="/2023/03/25/2303252216/9.png"></p>
<ul>
<li>并且在40%-60%缺失区域的人脸修复中效果也还不错。</li>
</ul>
<p><img src="/2023/03/25/2303252216/10.png"></p>
<p><img src="/2023/03/25/2303252216/11.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-20T11:28:02.000Z" title="2023/3/20 19:28:02">2023-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-25T13:59:29.511Z" title="2023/3/25 21:59:29">2023-03-25</time></span><span class="level-item">4 minutes read (About 611 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/20/2303201927/">Paper | CoordFill, Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying | AAAI2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title： <code>CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying</code></p>
</li>
<li><p>Keyword：<strong>Adaptive neural networks</strong></p>
</li>
<li><p>Idea：Parameterized Coordinate Querying</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2023年3月15号arXiv submitted，AAAI2023 accepted。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.08524">CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying (arxiv.org)</a></li>
<li>Code，作者还没更新repo。<a target="_blank" rel="noopener" href="https://github.com/NiFangBaAGe?tab=repositories">NiFangBaAGe (NiFangBaAGe) &#x2F; Repositories (github.com)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>高分辨率图像修复现存的问题：</p>
<ul>
<li>高分辨率图像需要较大的感受野，造成更多的计算量。</li>
<li>encoder-decoder架构会同时<strong>重建背景像素</strong>（非受损区域），作者认为这样会降低效率。</li>
</ul>
<p>贡献点：</p>
<ul>
<li>首次将continuous implicit representation引入高分辨率图像修复。</li>
<li>在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.02992">Spatially-Adaptive Pixelwise Networks for Fast Image Translation (arxiv.org)</a>的基础上，提出基于注意力快速傅里叶卷积（<strong>Attentional FFC</strong>）的参数生成网络。并且<strong>只针对掩膜区域进行重建</strong>。</li>
</ul>
<p><img src="/2023/03/20/2303201927/1.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/20/2303201927/2.png"></p>
<ul>
<li><p>先将高分辨率图像（e.g. 1024×1024）双线性下采样到低分辨率空间（e.g. 256×256）。Encoder为三层卷积层，将输入图片特征继续下采样，然后利用六个基于注意力的傅里叶卷积块生成最终的逐像素MLPs参数。</p>
</li>
<li><p>利用一个简单的线性映射，只选取孔洞区域的映射，并将目标分辨率作为条件输入使网络能够对于分辨率敏感。最后利用最近邻插值将逐像素MLPs参数上采样到高分辨率空间，加入带有高频信息的正弦信号位置编码，仅生成代填补区域像素，最终直接贴回原图。</p>
</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p><img src="/2023/03/20/2303201927/3.png"></p>
<ul>
<li>主打一个快，但是增益是之前CVPR21的I2I提出的，把该方法迁移到了Inpainting任务上而已。</li>
</ul>
<p><img src="/2023/03/20/2303201927/4.png"></p>
<ul>
<li><p>可能对于高分辨率图像修复的<strong>实际应用部署</strong>会有很大的帮助。</p>
</li>
<li><p>在人脸图像上也做了实验，但是与HiFill保持训练一致，训练mask只采取了25%左右的缺失区域，很难认定这是高分辨率图像修复任务。</p>
</li>
</ul>
<p><img src="/2023/03/20/2303201927/5.png"></p>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><blockquote>
<p>灌水痕迹明显的一篇AAAI。</p>
<p>但可能是因为作者是工业界的，用了八卡a100训练这样的模型，方便实际中的高分辨率图像修复部署吧。科研上的价值不大。</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-20T11:17:32.000Z" title="2023/3/20 19:17:32">2023-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-20T11:26:34.196Z" title="2023/3/20 19:26:34">2023-03-20</time></span><span class="level-item">4 minutes read (About 541 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/20/2303201917/">Paper | Spatially-Adaptive Pixelwise Networks for Fast Image Translation | CVPR2021</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Spatially-Adaptive Pixelwise Networks for Fast Image Translation</code></li>
<li>Keyword：<strong>Adaptive neural networks</strong>，Trainable efficient &amp; Gan-based I2I</li>
</ul>
<blockquote>
<p>Trainable efficient image transformations：Computationally heavy<br>inference is performed at extreme low-resolution, while the<br>high-resolution synthesis is comparatively lightweight. </p>
<p>计算量大的推理在极低分辨率下执行，而高分辨率合成更轻量级。</p>
</blockquote>
<ul>
<li>Idea：Pixel-specific Lightweight MLPs</li>
<li>Source<ul>
<li>Paper，2020年12月arXiv submitted，CVPR2021 accepted。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.02992">Spatially-Adaptive Pixelwise Networks for Fast Image Translation (arxiv.org)</a></li>
<li>Code，<a target="_blank" rel="noopener" href="https://tamarott.github.io/ASAPNet_web/">ASAPNet (tamarott.github.io)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>存在的问题：</p>
<ul>
<li>现有的I2I方法随着性能的提升，推理时间也不断增加。</li>
</ul>
<p><img src="/2023/03/20/2303201917/1.png"></p>
<p>本文的贡献：</p>
<ul>
<li>在不牺牲性能的同时，降低运行时间，提升推理速度。</li>
</ul>
<p><img src="/2023/03/20/2303201917/2.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/20/2303201917/3.png"></p>
<p>直觉上直接使用MLP预测效果会很差，文章提出了三个组件使网络表达能力增强。</p>
<ul>
<li><strong>Spatially-varying Parameters</strong>：与CNN的参数共享不同，本方法针对每个像素点的MLP参数是不同的（spatial-adaptive、spatial-varying）。逐像素预测可以并行计算，因为像素与像素之间独立，因此推理速度大幅度提升。</li>
<li><strong>Sinusoid Positional Encoding</strong>：除了input pixel values以外，还将像素空间位置以不同频率的正弦信号编码，其频率高于上采样因子，以生成高频图像细节。</li>
<li><strong>Input-adaptive Parameters</strong>：在低分辨率空间预测参数，对于每张图预测的逐像素MLPs参数均不同。</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>主打一个快，且性能没有显著下降。</li>
</ul>
<p><img src="/2023/03/20/2303201917/4.png"></p>
<ul>
<li><strong>MLPs一般只能学习到低频细节</strong>。在连续隐式表达中（<strong>神经渲染方向</strong>），MLPs表征能力不是很强，就像消融实验中的没有位置编码的模型生成结果一样。所以本文牺牲了极大的训练内存来换取速度。</li>
</ul>
<p><img src="/2023/03/20/2303201917/5.png"></p>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><ul>
<li>听师兄说，这是20年、21年CVPR比较热门的（灌水）方法。</li>
<li>后续会follow一篇在23年利用该方法发表（灌水）AAAI23的高分辨率图像修复方法。</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-17T06:05:11.000Z" title="2023/3/17 14:05:11">2023-03-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-17T06:15:32.745Z" title="2023/3/17 14:15:32">2023-03-17</time></span><span class="level-item">5 minutes read (About 682 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/17/2303171405/">Growth | Be a Techno-Optimist</a></p><div class="content"><p>All human progress is about overcoming an obstacle. <strong>From the wheel to the internet</strong>, we have discovered and invented our way out of all sorts of trouble. The story of science and technology is, in the main, one of making our lives easier.</p>
<p>Instead of talking out of both sides of our mouths, perhaps it is time that we were appreciated just how much we need technology, and how far it has helped us along. That is exactly what philosopher John Danaher does in his recent paper, Techno-optimism: an Analysis, an Evaluation and a Modest Defense.</p>
<p>Obviously, technology is not perfect. Smart phone addiction does exist, environmental destruction is happening, and we are each seeing a rapid, abrupt uprooting of how society has operated for millennia. If we say we’re “techno-optimists,” we are not saying that we are blind to technology’s problems. <strong>Optimism is not fanaticism</strong>.</p>
<p>Instead, as Danaher argues, optimism is defined by three elements. <strong>First</strong>, optimists believe “the good prevails over the bad by some distance, with that distance varying depending on the strength of the optimistic stance.” So, in terms of technology, it means <strong>the good outweighs the bad</strong>.</p>
<p><strong>Second</strong>, optimism tends to associate with an “<strong>affirmation of improvement</strong>.” The year 2022 is a better time to live than 1880   —   or even 1980.</p>
<p><strong>Third</strong>, optimists (and pessimists, for that matter) must believe that we can actually <strong>measure</strong> “good” as a value to track. We can point to this or that technology and say, “These are examples of good things that could only be caused by technological improvement.”</p>
<p>According to Danaher, in order to properly justify and rationalize techno-optimism, we must do three things: <strong>Establish values, determine facts, and evaluate</strong>.</p>
<p><strong>Establish values</strong>. First, we must establish certain values as being “good.” For instance, a techno-optimist “might argue that it is wonderful that people have more disposable income and a richer set of consumer goods and services from which to choose.”</p>
<p><strong>Determine facts</strong>. Once we have established these values, then we have to present those facts that support the claim that technology provides them.</p>
<p><strong>Evaluate</strong>. <strong>We need to present the facts that defend values, but we also need to acknowledge facts that contradict those values as well</strong>. As mentioned above, technology does have its problems. It can impact our mental health, it ravages the environment, and it drastically upends what being human has always meant. The techno-optimist is the one who believes the good of technology outweighs the bad.</p>
<p>There are two major critiques of techno-optimism that Danaher addresses.</p>
<p>First, the “treadmill critique” argues that technology won’t constantly make the world better. We have become so accustomed to technology that we no longer appreciate it as “good” but rather expect it as the norm.</p>
<p>Danaher counters by suggesting that there exists some “values that are not subject to baseline adaptation.” He cites “longer lives, fewer life-threatening illness, and more equality of opportunity” as examples of “goods” that will always be good, regardless of how accustomed we are to them.</p>
<p>Second, the “unsustainability critique” is the idea that if “optimism depends on present or continued economic growth, it also depends on the continued technological exploitation of natural resources. All natural resources are finite and have some upper limit of exploitability.”</p>
<p>Danaher’s response is that “technology is becoming less exploitative over time.” As a technology improves, then growth “decouples” from exploitation.</p>
<p>You do not have to adopt a starry-eyed “technology-will-save-humanity” viewpoint to be a techno-optimist. It’s perfectly reasonable to suggest that there are many existing problems with technology, and that it, alone, is insufficient for good to prevail.</p>
<p>Instead, we might sympathize with Danaher’s “modest techno-optimism.” According to this view, “we have the power to create the right institutions for generating, selecting, and creating material technologies, and acting on that belief in a cautious and sensible manner can make it more likely that the good will prevail over the bad.”</p>
<p><strong>It’s a kind of techno-optimism that perhaps needs a bit of human optimism, too</strong>.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-15T08:51:48.000Z" title="2023/3/15 16:51:48">2023-03-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-15T09:04:38.458Z" title="2023/3/15 17:04:38">2023-03-15</time></span><span class="level-item">7 minutes read (About 1052 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/15/2303151651/">Trend | GPT-4 —— The winner takes all</a></p><div class="content"><h1 id="Image-and-Text-Multimodal"><a href="#Image-and-Text-Multimodal" class="headerlink" title="Image and Text Multimodal"></a>Image and Text Multimodal</h1><blockquote>
<p>We report the development of GPT-4, a <strong>large-scale</strong>, <strong>multimodal model</strong> which can <strong>accept image and text inputs and produce text outputs</strong>.</p>
</blockquote>
<ul>
<li>模型底层逻辑还是image+text input（融入多模态元素，更唬人一些？），但还是文本outputs（不过听说chatgpt plus版本已经可以有image output了，怀疑是一些命令的组合？就类似于上一篇微软刚提出的Vision Chatgpt的方式一样，将视觉模型作为tool模型，large-scale语言预训练作为agent模型）。</li>
<li><strong>支持输入更多的tokens</strong>（更个性化，更方便定制了，更task-specific了）</li>
<li>加了一些VQA的性能对比。</li>
</ul>
<h1 id="Professional-and-Academic-Benchmarks"><a href="#Professional-and-Academic-Benchmarks" class="headerlink" title="Professional and Academic Benchmarks"></a>Professional and Academic Benchmarks</h1><blockquote>
<p>While less capable than humans in many real-world scenarios, GPT-4 exhibits <strong>human-level performance on various professional and academic benchmark</strong>s, including passing a simulated bar exam with a score around the top 10% of test takers. </p>
</blockquote>
<ul>
<li>professional benchmarks</li>
</ul>
<blockquote>
<p>乱杀应试教育界，秒杀多少普通人。</p>
<p>AI for science提上日程吧，早日研究，然后自我替代（开玩笑，不过很期待这一天）。</p>
</blockquote>
<p><img src="/2023/03/15/2303151651/1.png"></p>
<blockquote>
<p>这GRE、leetcode水平，感觉我自己都要花点时间才能达到呢。</p>
</blockquote>
<ul>
<li><p>academic benchmark</p>
<p>已经叫做<strong>benchmark-specific tuning</strong>了，面向任务的DL调参侠瑟瑟发抖。</p>
<p><img src="/2023/03/15/2303151651/2.png"></p>
</li>
</ul>
<h1 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h1><p><img src="/2023/03/15/2303151651/3.png"></p>
<ul>
<li><p>这个part让我觉得，训练一个大模型需要好多方面的协调，包括</p>
<ul>
<li><strong>Pretraining</strong><ul>
<li>Compute cluster scaling</li>
<li><strong>Data</strong></li>
<li>Distributed training infrastructure</li>
<li>Hardware correctness</li>
<li>Optimization &amp; architecture</li>
<li><strong>Training run babysitting</strong></li>
</ul>
</li>
<li><strong>Long context</strong><ul>
<li>Long context research</li>
<li>Long context kernels</li>
</ul>
</li>
<li><strong>Vision</strong><ul>
<li>Architecture research</li>
<li>Compute cluster scaling</li>
<li>Distributed training infrastructure</li>
<li>Hardware correctness</li>
<li>Data</li>
<li>Alignment data</li>
<li>Training run babysitting</li>
<li>Deployment &amp; post-training</li>
</ul>
</li>
<li><strong>Reinforcement Learning &amp; Alignment</strong><ul>
<li><strong>Dataset contributions</strong></li>
<li><strong>Data infrastructure</strong></li>
<li>ChatML format</li>
<li><strong>Model safety</strong></li>
<li>Refusals</li>
<li>Foundational RLHF and InstructGPT work</li>
<li>Flagship training runs</li>
<li>Code capability</li>
</ul>
</li>
<li><strong>Evaluation &amp; analysis</strong><ul>
<li>OpenAI Evals library</li>
<li>Model-graded evaluation infrastructure</li>
<li>Acceleration forecasting</li>
<li>ChatGPT evaluations</li>
<li>Capability evaluations</li>
<li>Coding evaluations</li>
<li>Real-world use case evaluations</li>
<li>Contamination investigations</li>
<li>Instruction following and API evals</li>
<li>Novel capability discovery</li>
<li>Vision evaluations</li>
<li>Economic impact evaluation</li>
<li>Non-proliferation, international humanitarian law &amp; national security red teaming</li>
<li>Overreliance analysis</li>
<li>Privacy and PII evaluations</li>
<li>Safety and policy evaluations</li>
<li><strong>OpenAI adversarial testers</strong></li>
<li>System card &amp; broader impacts analysis</li>
</ul>
</li>
<li><strong>Deployment</strong><ul>
<li>Inference research</li>
<li>GPT-4 API &amp; ChatML deployment</li>
<li>GPT-4 web experience</li>
<li>Inference infrastructure</li>
<li>Reliability engineering</li>
<li>Trust &amp; safety engineering</li>
<li>Trust &amp; safety monitoring and response</li>
<li>Trust &amp; safety policy</li>
<li>Deployment compute</li>
<li>Product management</li>
</ul>
</li>
<li><strong>Additional contributions</strong><ul>
<li><strong>Blog post &amp; paper content</strong></li>
<li>Communications</li>
<li>Compute allocation support</li>
<li>Contracting, revenue, pricing, &amp; finance support</li>
<li>Launch partners &amp; product operations</li>
<li>Legal</li>
<li>Security &amp; privacy engineering</li>
<li>System administration &amp; on-call support</li>
</ul>
</li>
</ul>
</li>
<li><p>比较费人的小部门就是data和training部分（标粗显示的部分），然后就是领域专家给反馈（adversarial testers）。</p>
</li>
<li><p>算法部分Pretraining+long context+Vision+RL，测试部署Evaluation+deployment，以及后期各种市场、产品，都缺一不可，都很关键啊。不过能看到AI产品能够有今天，也是十分欣慰了，以前的AI都停留在弱弱弱弱AI的层面吧，好处是觉得自己学的东西真的能改变世界，学科真的有技术爆炸式的飞跃进展，坏处是自己好像没什么用处了（美滋滋，不过发展的尽头，不都是要被替代的吗？语言、教育、设计、律师、计算机、金融各行各业，不论是专业性的，还是需要想象力的艺术生成，好像AI在某种程度上已经击败了90%的人类了吧）。</p>
</li>
<li><p>3年前的自己还很有信念的All in AI，坚信Deep Learning，距离通用AI的出现或许真的不远咯。</p>
</li>
<li><p>目前的AI变强了，但还是<strong>辅助人类办公，提升效率的帮手</strong>，距离完全代替人类还有很长的路要走（甚至真正的商业化都比较麻烦？）。愈发认为，人类的情感、情绪价值，在当下变得更为宝贵、更难以替代一些。</p>
</li>
<li><p><strong>未来究竟是理性的胜利、还是感性的胜利，是机器的胜利、还是人类的胜利呢。</strong>如果有生之年能够见证的话，还挺让人期待的。</p>
</li>
<li><p><strong>不过当下，打不过就加入嘛！</strong></p>
</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><p><a target="_blank" rel="noopener" href="https://openai.com/product/gpt-4">GPT-4 (openai.com)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kA7FBZsT6SIvwIkRwFS-xw">GPT-4震撼发布：多模态大模型，直接升级ChatGPT、必应，开放API，游戏终结了？ (qq.com)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/gpt-4.pdf">gpt-4.pdf (openai.com)</a></p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-14T12:03:26.000Z" title="2023/3/14 20:03:26">2023-03-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-15T08:03:51.233Z" title="2023/3/15 16:03:51">2023-03-15</time></span><span class="level-item">5 minutes read (About 690 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/14/2303142003/">Paper | Visual ChatGPT Talking, Drawing and Editing with Visual Foundation Models | arXiv2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</code></li>
<li>Keyword：Large Language Model（LLM），Visual Foundation Model（VFM）</li>
<li>Idea：<strong>Prompt Engineering</strong></li>
<li>Source<ul>
<li>Paper，2023年3月8日ArXiv Submitted，微软亚洲研究院的一项新工作。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.04671">2303.04671] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models (arxiv.org)</a></li>
<li>Code，刚发布几天，目前已经有1万多标星了。<a target="_blank" rel="noopener" href="https://github.com/microsoft/visual-chatgpt">microsoft&#x2F;visual-chatgpt: VisualChatGPT (github.com)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>存在的问题：</p>
<ul>
<li><p><strong>大型语言模型</strong>如ChatGPT利用<strong>单一语言模态</strong>训练，因此处理视觉信息的能力非常有限。</p>
</li>
<li><p>相比较而言，<strong>视觉基础模型</strong>（VFM，Visual Foundation Models）在计算机视觉方面潜力巨大，因而能够理解和生成复杂的图像（如ViT、BLIP、Stable Diffusion等等）。VFM模型对输入-输出格式的苛求和固定限制，使得其在<strong>人机交互方面不如会话语言模型灵活</strong>。</p>
</li>
</ul>
<p>贡献：</p>
<ul>
<li>Prompt Engineering：将ChatGPT和多个SOTA视觉基础模型连接。</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>没有任何的训练，系统构成：</p>
<ul>
<li><p>Part 1 ChatGPT（直接利用大语言集成工具<strong>LangChain</strong>，调用OpenAI text-davinci-003 version）</p>
<p><img src="/2023/03/14/2303142003/1.png"></p>
</li>
<li><p>Part 2 PromptManager</p>
<p>构造了一个巨大的Prompt，把系统规则、视觉基础模型调用、历史对话、用户query、历史推理、中间结果都包含，简单来说就是<strong>指导ChatGPT怎么调用模型，什么时候调用，怎么处理结果</strong>。ChatGPT和VFMs之间沟通提到图片的时候使用的是随机生成的<strong>uuid（universally unique identifier）</strong>，两者之间是没有向量或者图片数据交互的。</p>
<p><img src="/2023/03/14/2303142003/2.png"></p>
<p><img src="/2023/03/14/2303142003/3.png"></p>
<p><img src="/2023/03/14/2303142003/4.png"></p>
</li>
<li><p>Part 3 VFMs（22个训练好的SOTA视觉基础模型，直接调用，利用4张V100就能全部部署）</p>
<p><img src="/2023/03/14/2303142003/5.png"></p>
</li>
</ul>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><ul>
<li>不是真正的多模态大模型，不过是普通玩家（小公司）可以尝试的Prompt Engineering。</li>
<li>训练一个多任务的large-scale视觉-语言模型应该非常消耗算力吧，23年3月15日发布的gpt4虽然没有公开详细的技术细节，但我觉得底层加了Vision QA，也就是Image-to-Text的能力，还是很难将I2I，T2I，I2T完全结合再一起的。</li>
<li>不过大力出奇迹，stack more layers，feed more data。</li>
<li>猜测GPT4背后的一些图像能力是靠这样的简单逻辑实现的。</li>
</ul>
<p><img src="/2023/03/14/2303142003/6.png"></p>
<p><img src="/2023/03/14/2303142003/7.png"></p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a target="_blank" rel="noopener" href="https://www.aminer.cn/research_report/640c27d07cb68b460fa0a702">视觉ChatGPT来了，微软发布，代码已开源 - 热点 - 科研解读 - AMiner</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613133999">visual-chatgpt: 训什么练，我直接prompt一把梭 - 知乎 (zhihu.com)</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-07T13:03:34.000Z" title="2023/3/7 21:03:34">2023-03-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-14T12:03:57.788Z" title="2023/3/14 20:03:57">2023-03-14</time></span><span class="level-item">a minute read (About 203 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/07/2303072103/">Frontend | Icarus主题美化</a></p><div class="content"><ol>
<li><p>为博客添加nest动态线条特效</p>
<p>在<code>themes\icarus\layout\layout.jsx</code>的<code>body</code>中添加如下代码，CDN可根据自己使用的修改。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span> color=<span class="string">&quot;30,144,255&quot;</span> opacity=<span class="string">&#x27;0.5&#x27;</span> zIndex=<span class="string">&quot;-1&quot;</span> count=<span class="string">&quot;150&quot;</span> src=<span class="string">&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;</span>&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure>

<p>除了通过CDN加载，也可以下载到本地使用，详见<a target="_blank" rel="noopener" href="https://github.com/hustcc/canvas-nest.js">官方文档</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">color=&quot;255,255,255&quot; opacity=&#x27;0.7&#x27; # 改成了自己喜欢的颜色</span><br></pre></td></tr></table></figure>
</li>
<li><p>Code Highlight </p>
<p>可以从这些code highlight中找自己喜欢的styles。<a target="_blank" rel="noopener" href="https://github.com/highlightjs/highlight.js/tree/9.18.1/src/styles">highlight.js&#x2F;src&#x2F;styles at 9.18.1 · highlightjs&#x2F;highlight.js (github.com)</a></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">article:</span></span><br><span class="line">    <span class="comment"># Code highlight settings</span></span><br><span class="line">    <span class="attr">highlight:</span></span><br><span class="line">        <span class="comment"># Code highlight themes</span></span><br><span class="line">        <span class="comment"># https://github.com/highlightjs/highlight.js/tree/master/src/styles</span></span><br><span class="line">        <span class="attr">theme:</span> <span class="string">xt256</span> <span class="comment"># 赛博朋克主题的暗黑code highlight</span></span><br><span class="line">        <span class="comment"># Show copy code button</span></span><br><span class="line">        <span class="attr">clipboard:</span> <span class="literal">true</span></span><br><span class="line">        <span class="comment"># Default folding status of the code blocks. Can be &quot;&quot;, &quot;folded&quot;, &quot;unfolded&quot;</span></span><br><span class="line">        <span class="attr">fold:</span> <span class="string">unfolded</span></span><br></pre></td></tr></table></figure>

<p>我的最爱：monokai</p>
</li>
</ol>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a target="_blank" rel="noopener" href="http://www.anticme.com/2021/03/26/icarus%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE/">icarus个性化配置 - Hongjie’s blog (anticme.com)</a></li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar1.png" alt="Vanessa Ni🛸🏴‍☠️"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Vanessa Ni🛸🏴‍☠️</p><p class="is-size-6 is-block">💗I am currently a first-year postgraduate student at Fudan University. My research focuses on computer vision👁️, especially image/face inpainting and image quality assessment. 💻I graduated from Tianjin University with a Bachelor&#039;s degree in Software Engineering, and acquired a little knowledge about 2d/3d object detection. I worked as an algorithm engineer intern at CATRC and Baidu. 📙I enjoy reading (especially psychology and philosophy), traveling🧳, photography📸, working out💪, and dancing💃 in my free time. 🚀I want to become an algorithm engineer or go to the U.S. for Ph.D. 3 years later.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JennyVanessa" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JennyVanessa"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Growth/"><span class="level-start"><span class="level-item">Growth</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Util/"><span class="level-start"><span class="level-item">Util</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T08:16:52.000Z">2023-04-04</time></p><p class="title"><a href="/2023/04/04/2304041616/">Paper | Perceptual Artifacts Localization for Inpainting | ECCV2022</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T08:11:21.000Z">2023-04-04</time></p><p class="title"><a href="/2023/04/04/2304041611/">Paper | High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling | ECCV2020</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-27T09:23:48.000Z">2023-03-27</time></p><p class="title"><a href="/2023/03/27/2303271723/">Paper | SmartBrush, Text and Shape Guided Object Inpainting with Diffusion Model | CVPR2023</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-25T14:16:24.000Z">2023-03-25</time></p><p class="title"><a href="/2023/03/25/2303252216/">Paper | Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning | AAAI2023</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-20T11:28:02.000Z">2023-03-20</time></p><p class="title"><a href="/2023/03/20/2303201927/">Paper | CoordFill, Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying | AAAI2023</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Growth/"><span class="tag">Growth</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/paper/"><span class="tag">paper</span><span class="tag">11</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Just be here now🎇</a><p class="is-size-7"><span>&copy; 2023 Vanessa Ni</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>