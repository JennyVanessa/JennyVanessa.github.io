<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Vanessa&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Vanessa&#039;s blog🎀"><meta name="msapplication-TileImage" content="/img/my_favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Vanessa&#039;s blog🎀"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Vanessa&#039;s blog"><meta property="og:url" content="https://jennyvanessa.github.io/"><meta property="og:site_name" content="Vanessa&#039;s blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jennyvanessa.github.io/img/og_image.png"><meta property="article:author" content="Vanessa Ni"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://jennyvanessa.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jennyvanessa.github.io"},"headline":"Vanessa's blog","image":["https://jennyvanessa.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Vanessa Ni"},"publisher":{"@type":"Organization","name":"Vanessa's blog","logo":{"@type":"ImageObject","url":{"text":"Just be here now🎇"}}},"description":""}</script><link rel="icon" href="/img/my_favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><script type="text/javascript" color="255,255,255" opacity="0.7" zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Just be here now🎇</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JennyVanessa"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-05T12:40:39.000Z" title="2023/3/5 20:40:39">2023-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-05T12:46:39.163Z" title="2023/3/5 20:46:39">2023-03-05</time></span><span class="level-item">4 minutes read (About 558 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/05/2303052040/">Paper | ZITS++ Image Inpainting by Improving the Incremental Transformer on Structural Priors | arXiv2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors</code></li>
<li>Keyword：Transformer, High resolution Image Inpainting</li>
<li>Idea：之前CVPR2022会议文章的期刊版本，做了一些小改进和其他的尝试。</li>
<li>Source<ul>
<li>Paper，2022年10月第一版，2023年2月23日第二版（新鲜出炉的）。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.05950">2210.05950] ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors (arxiv.org)</a></li>
<li>Code，<a target="_blank" rel="noopener" href="https://github.com/DQiaole/ZITS_inpainting">DQiaole&#x2F;ZITS_inpainting: Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (CVPR2022) (github.com)</a>，<a target="_blank" rel="noopener" href="https://dqiaole.github.io/ZITS_inpainting/">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (dqiaole.github.io)</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>ZITS存在的问题：</p>
<ul>
<li>ZITS中使用的canny边缘不能区分有意义的结构。在复杂环境中Canny边缘产生confusing textures而不是具有丰富信息的底层结构。</li>
</ul>
<p><img src="/2023/03/05/2303052040/1.png"></p>
<ul>
<li>深入研究不同的图像先验信息引导的高分辨率图像修复是必要的。</li>
<li>提升LaMa的纹理修复性能。</li>
</ul>
<p>贡献点：</p>
<ul>
<li>在原始的ZITS上（transformer-based的边缘和线框补充），又加入了许多不同先验的实验分析和讨论，最终发现L-Edges、线框和梯度先验结合效果最好。</li>
<li>将补全好的先验信息融合到修复网络中需要上采样，提出了一种Edge Non-Maximum Suppression（E-NMS），将冗余的边缘信息过滤掉（消除边界附近的模糊边缘）。</li>
<li>对于LaMa进行修改，加入了<strong>Large Kernel Attention</strong>以及修改模型设计。（增益：large receptive fields and scale invariance尺度不变性。we promote the <strong>maxpool as the mask resizing strategy</strong> of PatchGAN instead of the nearest in LaMa）</li>
</ul>
<p><img src="/2023/03/05/2303052040/2.png"></p>
<ul>
<li>提供了一个高分辨率图像数据集，HR-Flickr。</li>
</ul>
<p><img src="/2023/03/05/2303052040/3.png"></p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/05/2303052040/4.png"></p>
<ul>
<li>提出了learning-based边缘CATS取代原来用的canny边缘。并利用E-NMS（现有的算法）过滤不确定的边缘。最终使用的先验是CAT+线框（wireframe）+梯度。</li>
</ul>
<p><img src="/2023/03/05/2303052040/5.png"></p>
<ul>
<li><p>利用扩张卷积分解large Kernel，实验中取K&#x3D;21。</p>
</li>
<li><p>mask resize策略：maxpool取代nearest resize（稳定训练过程）</p>
</li>
</ul>
<p><img src="/2023/03/05/2303052040/6.png"></p>
<h1 id="Evalutaion"><a href="#Evalutaion" class="headerlink" title="Evalutaion"></a>Evalutaion</h1><ul>
<li>定量性能提升明显。</li>
</ul>
<p><img src="/2023/03/05/2303052040/7.png"></p>
<ul>
<li>定量效果也很好。</li>
</ul>
<p><img src="/2023/03/05/2303052040/8.png"></p>
<ul>
<li>人脸修复效果也很好。</li>
</ul>
<p><img src="/2023/03/05/2303052040/9.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-05T08:52:52.000Z" title="2023/3/5 16:52:52">2023-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-05T09:00:36.645Z" title="2023/3/5 17:00:36">2023-03-05</time></span><span class="level-item">13 minutes read (About 2019 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/05/2303051650/">Paper | Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding | CVPR2022</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding</code></li>
<li>Keyword：Transformer, High resolution Image Inpainting</li>
<li>Idea：<strong>Extract edges and contours with Transformer</strong>, Masking Positional Encoding</li>
<li>Source<ul>
<li>Paper，2022年3月submitted的，到现在已经一年过去了，accepted in CVPR2022。[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.00867">2203.00867] Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (arxiv.org)</a></li>
<li>Code，基于LaMa做的一些小改进。<a target="_blank" rel="noopener" href="https://github.com/DQiaole/ZITS_inpainting">DQiaole&#x2F;ZITS_inpainting: Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (CVPR2022) (github.com)</a>，<a target="_blank" rel="noopener" href="https://dqiaole.github.io/ZITS_inpainting/">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding (dqiaole.github.io)</a></li>
<li>PaperReading，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/496739824">CVPR2022|基于Transformer结构增强的增量式图像修复|ZITS - 知乎 (zhihu.com)</a>非常好的阅读笔记。</li>
</ul>
</li>
</ul>
<h1 id="Abstract-v1"><a href="#Abstract-v1" class="headerlink" title="Abstract v1"></a>Abstract v1</h1><p>本文是基于WACV’22的高分辨率图像修复工作LaMa进一步改进的，更偏向于<strong>自然场景的修复（更注重结构、轮廓的先验信息）</strong>。</p>
<p>现存的问题：</p>
<ul>
<li><p>1）现有的方法受限于CNN<strong>有限的感受野</strong>，只能处理常规的纹理，仍存在恢复生动纹理与合理的<strong>整体结构</strong>的问题（Vivid textures and Reasonable structures）。</p>
</li>
<li><p>2）Attention-based模型（Transformer）虽然能更好的学习长距离依赖（Long-range dependency），但是受限于高分辨率图像推理时的<strong>Heavy Computation</strong>。</p>
</li>
</ul>
<p>解决的方法（贡献）：</p>
<ul>
<li>1）【主要贡献】An additional structure restorer，增加一个额外的结构修复器，增量式的辅助图像修复。<ul>
<li>在固定的低分辨率Sketch space（Gray-scale space）修复整体的结构，并可以通过上采样融入到修复过程中。</li>
<li>Can be integrated with other pretrained inpainting models efficiently with the zero-initialized residual addition（无需额外训练，直接融入到其他Inpainting预训练模型中）。</li>
</ul>
</li>
<li>2）Masking positional encoding strategy用于提升使用Large irregular mask训练的性能。</li>
</ul>
<h1 id="Abstract-v2"><a href="#Abstract-v2" class="headerlink" title="Abstract v2"></a>Abstract v2</h1><p>现存的问题：</p>
<ul>
<li>现有的Inpainting方法只能处理regular textures，由于CNN感受野有限的问题，失去了对于图像<strong>整体结构（Holistic Structure）</strong>的把控。</li>
<li>基于attention的方法可以一定程度上解决该问题，但受限于高分辨率图像推理时的<strong>Heavy Computation</strong>。</li>
</ul>
<p>贡献：</p>
<ul>
<li>Motivation：对于高分辨率自然图像修复来说，边缘信息十分重要，如果没有对于大图像的整体理解，很难恢复场景的边缘和线条，尤其是纹理较弱的场景。Method：使用一个<strong>额外的结构恢复网络</strong>，增量式的辅助图像修复过程。具体而言：transformer-based网络，在固定的低分辨率草图空间中，修复图像的<strong>边缘和轮廓线条</strong>，而后上采样到高分辨率，融合到后续图像修复网络中。</li>
<li><strong>Zero-initialized Residual Addition</strong>（零初始化残差融合）增量训练策略：提出的方法可以和其他的pretrained inpainting model轻易的整合在一起（许多其他利用先验信息的方法通常是多阶段多模型，训练成本高，而这个策略可以在较少的step数中快速收敛）。</li>
<li>提出了一个<strong>Masking Positional Encoding Strategy</strong>，提升在大mask配置下的模型性能。（高分辨率、较大缺失区域的修复，模型前期会在mask区域重复产生没有语义的伪影，浪费计算量）</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>Image Inpainting Goal：The inpainted images should remain both <strong>semantically coherent textures</strong> and <strong>visually reasonable structures</strong>. 这里也给了我们一点点启发，对于人脸修复而言，语义一致性至关重要，所以利用语义分割信息来引导人脸修复是一个好的想法；而后者，整体结构的连贯性，则对于自然场景图像修复至关重要。</li>
<li>Image Inpainting任务现存的问题<ul>
<li>1）Limited receptive fields。面对large corrupted region和高分辨率图像时问题更加凸显。</li>
<li>2）Missing holistic structures。缺乏整体结构，Recovering key edges and lines for scenes。</li>
<li>3） Heavy computations。训练高分辨率图像的GAN非常tricky and costly。</li>
<li>4） No positional information in masked regions。在大mask配置下，模型会生成没有意义伪影，浪费计算量。</li>
</ul>
</li>
</ul>
<blockquote>
<p>很好，我的另一个Idea别人也已经实现了，好好看好好学吧(●’◡’●)</p>
</blockquote>
<ul>
<li>作者分析了LaMa的不足之处（其实非常明显），LaMa的本质是在频域内做了1×1卷积保证了相同周期性信号的关联，也就是LaMa作者想要解决的重复性纹理的修复。但是这样的方法<strong>无法确保整体结构</strong>，并且在<strong>纹理较弱的图像</strong>上性能很差。</li>
</ul>
<blockquote>
<p>最先使用transformer-based做low-resolution图像修复，然后再CNN上采样超分一下的工作。</p>
<ul>
<li><p>Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. High-fidelity pluralistic image completion with transformers. arXiv preprint arXiv:2103.14031, 2021.</p>
</li>
<li><p>Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying Ma, Xuansong Xie, and Chunyan Miao. Diverse image inpainting with bidirectional and autoregressive transformers. arXiv preprint arXiv:2104.12335, 2021.</p>
</li>
</ul>
<p>还有许多使用先验信息的网络，但通常都是多阶段图像修复，训练成本较高（trained from scratch）。</p>
</blockquote>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/03/05/2303051650/1.png"></p>
<ul>
<li>首先将mask、masked image（valid pixel为1，待填充区域为0，mask可视化时反转一下，待填充变为1，都是为了方便后续计算）、canny边缘提取器获得的masked edge（边缘）以及利用作者之前提出的模型获取的masked lines（线框，主要是建模两点之间的连线，所以上采样下采样时不存在歧义，但是canny边缘提取出来的信息在不同feature size提取出的边缘可能不同）。</li>
<li>送入TSR，首先将256×256的图片下采样三次到32×32大小，然后利用基于轴向注意力和常规注意力的transformer，减少计算量提升计算效率，最后获得256×256的修复后的边缘和线框。后续利用一个简单的四层CNN网络来对于修复好的先验信息进行上采样，只用线框数据进行训练而不用线框加边缘数据，这样做能够更好的消除歧义，获得不同分辨率更加一致的先验信息。</li>
<li>因为边缘和线框信息是稀疏的，所以利用基于门控卷积的网络来提取更关键的信息，并采用多尺度信息，也就是中间block的最后一层和上采样的三层，通过零初始化残差融合（就是做了一个简单的残差运算），和baseline LaMa的前四层融合在一起，然后训练50k进行一个增量学习微调就能显著的提升原模型的效果。</li>
<li>至于MPE（Masking Positional Encoding），其实就是取一个3×3的all-one卷积核来和mask区域做计算，能够获得距离大mask中心的距离信息以及mask方向信息，送入到baseline网络中作为辅助信息。（黑色区域为1白色为0，很简单的卷积运算）。</li>
</ul>
<p><img src="/2023/03/05/2303051650/2.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>主要针对自然场景图像修复，定性上的性能增益不是很明显。</li>
</ul>
<p><img src="/2023/03/05/2303051650/3.png"></p>
<p><img src="/2023/03/05/2303051650/4.png"></p>
<ul>
<li>MPE这个方法更是鸡肋，出发点很好但是做的太简单了，所以也没有多高的性能增益。</li>
</ul>
<p><img src="/2023/03/05/2303051650/5.png"></p>
<ul>
<li>但是定性效果很好，主要是整体结构信息（边缘和线框）对于高分辨率的自然场景图像来说是非常关键的信息。作者之前提出的提取线框的模型，我觉得底层逻辑就像是透视图，对于空间布局来说，透视图很重要，所以修复出来的图片效果会更好。</li>
</ul>
<p><img src="/2023/03/05/2303051650/6.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-28T11:21:54.000Z" title="2023/2/28 19:21:54">2023-02-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-02-28T11:43:28.023Z" title="2023/2/28 19:43:28">2023-02-28</time></span><span class="level-item">5 minutes read (About 732 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/28/2302281930/">Issues | Baseline MISF-CVPR2022 Reprod &amp; GIQA improve</a></p><div class="content"><ul>
<li>Official code：<a target="_blank" rel="noopener" href="https://github.com/tsingqguo/misf">tsingqguo&#x2F;misf (github.com)</a></li>
</ul>
<h1 id="关于loss参数"><a href="#关于loss参数" class="headerlink" title="关于loss参数"></a>关于loss参数</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;epoch&#x27;, 1), (&#x27;iter&#x27;, 1), (&#x27;l_d2&#x27;, 0.707538366317749), (&#x27;l_g2&#x27;, 0.07427514344453812), (&#x27;l_l1&#x27;, 0.7772688865661621), (&#x27;l_per&#x27;, 0.20167401432991028), (&#x27;l_sty&#x27;, 0.393798291683197)]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logs = [</span><br><span class="line">            (<span class="string">&quot;l_d2&quot;</span>, dis_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_g2&quot;</span>, gen_gan_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_l1&quot;</span>, gen_l1_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_per&quot;</span>, gen_content_loss.item()),</span><br><span class="line">            (<span class="string">&quot;l_sty&quot;</span>, gen_style_loss.item()),</span><br><span class="line">        ]</span><br></pre></td></tr></table></figure>

<p>其中，l_d2是Inpainting Model的Discriminator loss，l_g2是Inpainting Model的Generator loss，l_l1是L1 loss，l_per是Perceptual loss，l_sty是Style loss。</p>
<p>这篇文章作者的code是基于Edge Connect的代码Repo的，原模型Edge Connect分为了Edge model、Inpainting Model、Inpaint with Edge Model以及Joint Model四个训练阶段，这里MISF的作者应该是只用了Inainting Model的部分并进行了修改。</p>
<h1 id="wandb使用"><a href="#wandb使用" class="headerlink" title="wandb使用"></a>wandb使用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb  <span class="comment"># 使wandb库在pytorch库之后引用</span></span><br><span class="line"></span><br><span class="line">default_config = <span class="built_in">dict</span>(</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    dropout=<span class="number">0.5</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">wandb.init(project=<span class="string">&quot;pj-name&quot;</span>, config=default_config, mode=<span class="string">&quot;online/offline/disabled&quot;</span>)</span><br><span class="line"></span><br><span class="line">batch_size = wandb.config.batch_size <span class="comment"># 保证代码可读性和一致性</span></span><br><span class="line"></span><br><span class="line">wandb.log(&#123;<span class="string">&#x27;epoch&#x27;</span>: epoch, <span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;accuracy&#x27;</span>: accuracy&#125;)</span><br></pre></td></tr></table></figure>

<h1 id="Package-import"><a href="#Package-import" class="headerlink" title="Package import"></a>Package import</h1><p><code>sys.path</code>指定模块搜索路径的<strong>列表</strong>。默认情况下，<code>python</code>导入文件或者模块，会在<code>sys.path</code>里找模块的路径。如果路径下搜索不到模块的话，就会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="built_in">print</span>(sys.path)</span><br><span class="line">sys.path.append(<span class="string">&#x27;/home/nsy/nlp&#x27;</span>) <span class="comment"># package路径为/home/nsy/nlp/new_package</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;/home/user5/code/misf-main&#x27;, &#x27;/home/user5/.pycharm_helpers/pydev&#x27;, &#x27;/home/user5/code/misf-main&#x27;, &#x27;/home/user5/.pycharm_helpers/pycharm_display&#x27;, &#x27;/home/user5/.pycharm_helpers/third_party/thriftpy&#x27;, &#x27;/home/user5/.pycharm_helpers/pydev&#x27;, &#x27;/home/user5/code/misf-main/C&#x27;, &#x27;/Users/75796/AppData/Local/JetBrains/PyCharm2021.3/cythonExtensions&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python38.zip&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8/lib-dynload&#x27;, &#x27;/home/user5/.local/lib/python3.8/site-packages&#x27;, &#x27;/home/user5/code/PUT-main&#x27;, &#x27;/home/user5/anaconda3/envs/testenv/lib/python3.8/site-packages&#x27;, &#x27;/home/user5/.pycharm_helpers/pycharm_matplotlib_backend&#x27;]</span><br></pre></td></tr></table></figure>

<h1 id="后台训练"><a href="#后台训练" class="headerlink" title="后台训练"></a>后台训练</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -u main.py &gt;02272115_loss.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h1 id="GIQA升级版FIQA"><a href="#GIQA升级版FIQA" class="headerlink" title="GIQA升级版FIQA"></a>GIQA升级版FIQA</h1><ul>
<li>Best model？</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- checkpoints/acc01090300/model_best.pth.tar</span><br><span class="line">- /home/user5/code/QA/GIQA-master/MBC-GIQA/checkpoints/acc01090300/model_best.pth.tar</span><br></pre></td></tr></table></figure>

<ul>
<li>Freeze pretrained layer</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/311095447">(29 封私信 &#x2F; 7 条消息) Pytorch 如何精确的冻结我想冻结的预训练模型的某一层，有什么命令吗？ - 知乎 (zhihu.com)</a></p>
<h1 id="算法工程师升级打怪"><a href="#算法工程师升级打怪" class="headerlink" title="算法工程师升级打怪"></a>算法工程师升级打怪</h1><ul>
<li><p>成为一个算法工程师首先你得有工程能力，就是说你得先能干活，<strong>熟练的掌握一门编程语言</strong>必不可少；</p>
</li>
<li><p>然后是<strong>相关领域的专业知识</strong>，比如推荐算法，你需要了解常见推荐算法的原理、优缺点、应用场景等；</p>
</li>
<li><p>然后是<strong>机器学习的基础知识</strong>，李航的《统计机器学习》，周志华的《机器学习》，Benjio的《深度学习》，这三本书至少得过个那么一两遍吧，把基础知识掌握牢了再学习其它的就容易多了，基础不牢地动山摇；</p>
</li>
<li><p>然后是掌握一些<strong>数据结构和算法知识</strong>，这个还是比较重要的，对你写出高效的代码很有帮助。</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-26T14:47:11.000Z" title="2023/2/26 22:47:11">2023-02-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-02T06:32:05.730Z" title="2023/3/2 14:32:05">2023-03-02</time></span><span class="level-item">8 minutes read (About 1234 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/26/2302262250/">Paper | Resolution-robust Large Mask Inpainting with Fourier Convolutions | WACV2022</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li>Title： <code>Resolution-robust Large Mask Inpainting with Fourier Convolutions</code></li>
<li>Keyword：Large Mask Inapinting</li>
<li>Idea：Fourier Convolutions</li>
<li>Source<ul>
<li>Paper，2021年9月15日submitted的。最后发表在WACV2022上，确实是Applications of CV，非常实用。后续有很多CVPR2022的高分辨率图像修复任务都和这篇工作做了对比。[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.07161">2109.07161] Resolution-robust Large Mask Inpainting with Fourier Convolutions (arxiv.org)</a></li>
<li>Code，大分辨率图像修复效果非常好的一项工作，面向落地的。<a target="_blank" rel="noopener" href="https://github.com/saic-mdal/lama%EF%BC%8C[Resolution-robust">https://github.com/saic-mdal/lama，[Resolution-robust</a> Large Mask Inpainting with Fourier Convolutions (advimman.github.io)](<a target="_blank" rel="noopener" href="https://advimman.github.io/lama-project/">https://advimman.github.io/lama-project/</a>)</li>
<li>Vedio，超棒的一个paper讲解，非作者本人，但是邀请了一作来interview。<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Lg97gWXsiQ4">Resolution-robust Large Mask Inpainting with Fourier Convolutions (w&#x2F; Author Interview) - YouTube</a></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现存的问题：</p>
<ul>
<li>Modern image inpainting systems, often struggle with <strong>large missing areas</strong>, <strong>complex geometric structures</strong>, and <strong>high-resolution images</strong>. 目前图像修复存在的问题有：大缺失区域（但个人认为ill-posed problem不是傅里叶卷积能够解决的）、复杂几何结构以及高分辨率图像修复。</li>
</ul>
<p>猜想：</p>
<ul>
<li>如何解决这个问题？作者认为最主要的原因是lack of an <strong>effective receptive field</strong> in both the inpainting <strong>network and the loss function</strong>.</li>
</ul>
<p>本文LaMa（Large mask inpainting）贡献点：</p>
<ul>
<li>在网络结构上，使用fast Fourier convolutions的inpainting network architecture，image-wide的感受野（快速傅里叶卷积的贡献）。</li>
<li>在损失函数上，A high receptive field perceptual loss。</li>
<li>在训练策略上，使用Large training mask。</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>A large effective receptive field</strong> is essential for understanding the global structure of an image.</p>
<ul>
<li>第一， <strong>high receptive field architecture</strong>。文章提出了基于快速傅里叶卷积（FFCs）的网络架构，能够使得网络前几层感受野都能cover整个图像。可以提升perceptual quality并使网络轻量化，而且泛化能力很强（即使训练集不包含的高分辨率图像，也能很好的推理）。</li>
<li>第二， <strong>high receptive field loss function</strong>。文章提出基于语义分割网络、大感受野的perceptual loss。能够提升全局结构和形状的一致性。</li>
<li>第三，<strong>aggressive algorithm of training masks generation</strong>。training mask generation，生成更大的mask。</li>
</ul>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>大mask配置下，如果依旧利用传统的3×3ResNet卷积核，在网络前期感受野可能位于掩膜内部，所以网络中的许多层都缺乏全局上下文，浪费了计算量和参数。</p>
<h2 id="Add-FFC"><a href="#Add-FFC" class="headerlink" title="Add FFC"></a>Add FFC</h2><ul>
<li><p>而Fast Fourier convolution (FFC) 能够让网络前几层应用全局的上下文信息。包含两个并行分支，1）局部分支使用常规的卷积操作；2）全局分支使用real FFT，作用在实数信号上。FFT会转换到复数空间（频域）。而inverse real FFT能够保证输出是实数。</p>
</li>
<li><p>这里简单的real FFT得到的复数实部和虚部concat到了一起，然后在频域上做了一个1×1卷积，也就是同频分量的卷积，这样能保证周期性信号的修复（也就是重复性的pattern，作者最初的motivation就是认为现有的方法对于重复性pattern修复的结果不佳，想到重复pattern就想到了周期性信号，也就使用了FFT来解决这个问题）</p>
</li>
</ul>
<p><img src="/2023/02/26/2302262250/1.png"></p>
<p><img src="/2023/02/26/2302262250/2.png"></p>
<p><img src="/2023/02/26/2302262250/3.png"></p>
<ul>
<li>提出了一个Fast Fourier Conv Residual Block，也就是res block改成快速傅里叶卷积。FFC还有局部分支和全局分支的交互，作用在每一个层之间。</li>
</ul>
<p><img src="/2023/02/26/2302262250/4.png"></p>
<h2 id="Perceptual-loss-pro"><a href="#Perceptual-loss-pro" class="headerlink" title="Perceptual loss pro"></a>Perceptual loss pro</h2><ul>
<li>在鉴别器部分，使用segmentation model作为backbone来专注于high-level information，而不是classification model backbone，更专注于纹理等低级特征。使用傅里叶或扩张卷积来实现均可。</li>
</ul>
<p><img src="/2023/02/26/2302262250/5.png"></p>
<ul>
<li>这里做了消融实验验证了对于perceptual loss升级后的效果。因为生成器更关注于全局信息，所以也要使判别器的性能提升，这样在GAN的训练过程中才能保持平衡。</li>
</ul>
<p><img src="/2023/02/26/2302262250/6.png"></p>
<h2 id="Generation-of-large-mask"><a href="#Generation-of-large-mask" class="headerlink" title="Generation of large mask"></a>Generation of large mask</h2><ul>
<li>输入的数据对于模型的性能提升很重要。与deepfillv2和narrow mask相比，文章生成large wide mask（多边形宽笔划）和large box mask的组合，作为训练输入。</li>
</ul>
<p><img src="/2023/02/26/2302262250/7.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>红色代表本方法比其他方法性能提升的百分比。可以看出在narrow mask配置下，本文方法超过绝大多数method，但是在wide mask配置下，性能吊打其他方法。</li>
</ul>
<p><img src="/2023/02/26/2302262250/8.png"></p>
<ul>
<li>使用傅里叶卷积的消融实验，在narrow mask下傅里叶卷积模型的性能提升效果不是很明显，但是大mask配置下优势就很突出。</li>
</ul>
<p><img src="/2023/02/26/2302262250/10.png"></p>
<ul>
<li>还可以泛化到高分辨率图像上。</li>
</ul>
<p><img src="/2023/02/26/2302262250/9.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-24T05:56:13.000Z" title="2023/2/24 13:56:13">2023-02-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-02-24T07:03:54.309Z" title="2023/2/24 15:03:54">2023-02-24</time></span><span class="level-item">6 minutes read (About 919 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/24/2302241356/">Backend | 关于Node.js、NPM和Node_modules</a></p><div class="content"><p><img src="https://preview.redd.it/tfugj4n3l6ez.png?width=960&crop=smart&auto=webp&v=enabled&s=e6809adf6214bff75cce6a5a37469f1d8c3d7087" alt="r/ProgrammerHumor - Sun Neutron star Black hole node_modules HEAVIEST OBJECTS IN THE UNIVERSE"></p>
<h1 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h1><p>首先看了一下Node.js官网的介绍。</p>
<blockquote>
<p>As an asynchronous event-driven <strong>JavaScript runtime</strong>, Node.js is designed to <strong>build scalable network applications</strong>. </p>
</blockquote>
<p>说的都是什么鬼话，一句没看明白。让我们来看看人话是什么样的。</p>
<blockquote>
<p>Node.js, which is a <strong>run-time environment</strong> that includes everything required to execute a program written in JavaScript.</p>
<p>Node.js is neither a programming language nor a framework; it’s an environment for them.</p>
</blockquote>
<ul>
<li>Node.js是用JavaScript写程序时的一个运行时环境。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.npmjs.com/">NPM</a> is Node.js’s package ecosystem. It is the largest ecosystem of all open-source libraries in the world, with over 1 million packages and growing. NPM is free to use, and thousands of open source developers contribute to it daily.</p>
</blockquote>
<ul>
<li>NPM是Node.js的包生态系统，也就是管理package的。这是世界上最大的一个开源库生态，据说每天都会有200多个新的package被注册。</li>
<li>根据项目中的package.json或package-lock.json文件，利用<code>npm install</code>就可以安装项目所有的依赖库，并存储在node_modules下。</li>
</ul>
<h1 id="与Python库管理的区别"><a href="#与Python库管理的区别" class="headerlink" title="与Python库管理的区别"></a>与Python库管理的区别</h1><ul>
<li><p>npm vs. pip</p>
<ul>
<li><p>npm使用的是局部依赖，所以相同的module会被反复安装到每个项目以及每个可传递的依赖项上（ The same module is installed over and over again for every project and every transitive dependency）。一个package可以是一个tar包，也可以是本地file协议，甚至git仓库地址。所以，node_module——HEAVIEST OBJECTS IN THE UNIVERSE。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node_modules</span><br><span class="line">	mod-a</span><br><span class="line">		node_modules</span><br><span class="line">        	mod-b@<span class="number">1.0</span></span><br><span class="line">	mod-c</span><br><span class="line">		node_modules</span><br><span class="line">        	mod-b@<span class="number">2.0</span></span><br><span class="line">	mod-d</span><br><span class="line">		node-modules</span><br><span class="line">			mod-b@<span class="number">2.0</span></span><br></pre></td></tr></table></figure>

<p>虽然mod-c和mod-b依赖同一个mod-b版本，但是该版本却安装了两遍。如果应用了很多第三方库，同时第三方库依赖了一些很基础的第三方库（如lodash），node_modules里就会充满各种重复版本的lodash。</p>
</li>
<li><p>而pip使用的全局依赖（至少对于虚拟环境而言是全局依赖的），所以就避免了上述问题。</p>
</li>
</ul>
</li>
<li><p>standard library python vs. JS</p>
<ul>
<li>Python的标准库比较大，与JS的标准库相比。</li>
<li>所以JS会依赖更多的packages。</li>
</ul>
</li>
<li><p>我觉得可以把node.js类比于anaconda，都像环境和容器一样。</p>
</li>
</ul>
<h1 id="ICARUS的npm版本和git版本"><a href="#ICARUS的npm版本和git版本" class="headerlink" title="ICARUS的npm版本和git版本"></a>ICARUS的npm版本和git版本</h1><p>首先我的blog是基于hexo的。</p>
<ul>
<li>因为有两个版本可以安装，npm install下来的就是直接到node_modules里，其实就是github repo的一个注册包版本（node_modules&#x2F;hexo-theme-icarus包含package.json，所以它是一个package而不是一个module？不过我的觉得package和module的区分不重要）。</li>
<li>而git clone安装方法是存在themes文件夹下面。</li>
</ul>
<p>如果我想修改一些主页的设置，就需要改主题的源代码，但是他是以npm的方式安装的，虽然直接修改也能生效（因为是本地路径查询包，所以直接修改node_modules中的库也是没问题的吧？），但是这种方法十分的不优雅（比如某天重新装了一下node_modules就全G了）。</p>
<p>推荐的方法是利用git clone安装到theme文件下，也就是自己的项目里，然后修改好了闲的没事的话可以注册到npm上，这样别人也能使用你修改后的icarus plus版本了，而且npm直接安装一下，十分的方便。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-23T13:50:51.000Z" title="2023/2/23 21:50:51">2023-02-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-02T06:32:52.706Z" title="2023/3/2 14:32:52">2023-03-02</time></span><span class="level-item">7 minutes read (About 1011 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/23/2302232150/">Paper | SFI-Swin Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions | arXiv2023</a></p><div class="content"><h1 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h1><ul>
<li><p>Title：<code>SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions</code></p>
</li>
<li><p>Keyword：Face Inpainting、Swin Transformer</p>
</li>
<li><p>Idea：Symmetric（对称的，人脸对称性）、Distinctly Learning Face Components Distributions（显式学习面部组件分布）</p>
</li>
<li><p>Source</p>
<ul>
<li>Paper，2023年1月9号Submitted到arxiv上的。[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.03130">2301.03130] SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions (arxiv.org)</a></li>
<li>Code，Repo给出了但是代码还没有push上来。<a target="_blank" rel="noopener" href="https://github.com/mohammadrezanaderi4/SFI-Swin">mohammadrezanaderi4&#x2F;SFI-Swin: SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions https://arxiv.org/abs/2301.03130 (github.com)</a></li>
</ul>
</li>
</ul>
<blockquote>
<p>日常感叹，为什么我能想到的Idea别人总能如此之快的抢发。当我还在拖拖拉拉实现Idea，别人已经验证完了。要多读paper，更重要的是多写code，实现Idea并验证哇。世界上最遥远的距离就是<strong>知道和做到</strong>。</p>
</blockquote>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现存的问题（问题陈述）：</p>
<ul>
<li><p>None of the existing inpainting methods consider the <strong>symmetry and homogeneity</strong> of the picture. </p>
<p>现有的方法在人脸修复的过程中没有考虑图像的<strong>对称性和同质性</strong>。</p>
</li>
<li><p>The metrics that assess a repaired face image quality cannot measure the <strong>preservation of symmetry</strong> between the rebuilt and existing parts of a face. </p>
<p>现有的评估指标无法<strong>衡量修复人脸的对称性</strong>。</p>
</li>
</ul>
<p>提出的方法（贡献点）：</p>
<ul>
<li>利用多discriminators分别验证五官的生成质量（提升对人脸高级语义五官的理解），构建一个transformer-based的网络（大感受野能够保证面部对称性）。</li>
<li>提出symmetry concentration score指标，来评估修复人脸的对称性。</li>
<li>在reality, symmetry, and homogeneity三个维度上，比最近提出的sota算法效果好。</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>在图像处理中，同质性指的是测量图像的局部均匀性。</li>
<li>文章中的同质性指的是修复的缺失区域需要和面部的其他区域保持协调（global features of each part of the face）。The inpainted regions must be homogeneous with the other parts of the face and highly correlated to the available surrounding areas of the input image.</li>
<li>对称性指的是面部的左右对称。facial symmetry must be preserved between the left and right sides.</li>
</ul>
<p>作者认为现存方法的问题出在了<strong>损失</strong>函数无法向生成器传达<strong>面部特征的整体理解</strong>。This shortcoming is because the network losses do not convey a general understanding of the facial features to the generator. </p>
<p>于是作者分析了主流Inpainting方法常用的几种loss对于模型训练的影响，包括pixel-wise, adversarial, feature-matching, and perceptual loss。</p>
<ol>
<li>pixel-wise loss。L1、L2范数，只能让网络理解到底层特征（low-level features）。👉focus on 底层特征（颜色、纹理）</li>
<li>adversarial loss。能够让gt和生成图像的分布（distribution）接近，使用discriminator和generator构成博弈；feature-matching loss。gt和pred作为输入，提取discriminator中间层特征。这两个loss只能让生成的图片看起来真实，但不能保证missing regions exactly similar to ground truth（inpainting任务的不适定性，ill-posed problem），大多数鉴别器是patch-based的，所以只能保证局部真实感。👉focus on 生成patches内容的真实感</li>
<li>perceptual loss。先利用一个seg network的预训练提取高级语义特征，然后计算L1、L2范数。主要考虑了high-level features，比如边缘。👉focus on 边缘轮廓的平滑性</li>
</ol>
<blockquote>
<p>一般是过一个类似VGG的backbone预训练提取特征，high-level features就默认为语义及以上层次的特征。</p>
</blockquote>
<p>有时上述的loss会牺牲面部对称性而达到局部真实感的最优，所以我们现在需要💡homogeneity-aware loss均匀感知损失，来约束模型。同时，transformer的大感受野也能保证面部对称性。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2023/02/23/2302232150/1.png"></p>
<p><img src="/2023/02/23/2302232150/2.png"></p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ul>
<li>方法效果一般，更多的是Swin transformer带来的加成。</li>
</ul>
<p><img src="/2023/02/23/2302232150/3.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-15T11:28:38.000Z" title="2023/2/15 19:28:38">2023-02-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-02-15T12:15:07.520Z" title="2023/2/15 20:15:07">2023-02-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Util/">Util</a></span><span class="level-item">4 minutes read (About 525 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/15/2302151930/">Util | Github教育认证以及Copilot使用</a></p><div class="content"><blockquote>
<p>下午浪费了一点时间踩坑，希望能够帮助到其他需要教育认证的朋友们~</p>
</blockquote>
<h1 id="Github教育认证"><a href="#Github教育认证" class="headerlink" title="Github教育认证"></a>Github教育认证</h1><ul>
<li><strong>不要挂梯子</strong>，直接用Microsoft Edge打开<a target="_blank" rel="noopener" href="https://education.github.com/">GitHub Education</a>，进行后续验证。</li>
<li><a target="_blank" rel="noopener" href="https://my.chsi.com.cn/archive/index.jsp">学信网</a>下载学籍报告，用<a target="_blank" rel="noopener" href="https://www.deepl.com/zh/translator/files">DeepL文档翻译</a>把报告<strong>翻译成英文版</strong>，截图保存为jpg格式。</li>
<li>直接上传图像，proof选择Other，填<code>Ministry of Education Online Verification Report of Student Status</code></li>
</ul>
<p>刚开始上传会提示profile问题、没有valid date问题等。我还把Github profile重新改了一遍，但这不是问题的关键。主要问题还是上传的学籍报告不是英文版or不清晰，之前用的是Google的文档翻译，翻译出来的字很小，再转成JPEG压缩了一下很模糊。</p>
<h1 id="Copilot使用"><a href="#Copilot使用" class="headerlink" title="Copilot使用"></a>Copilot使用</h1><ul>
<li>有了教育认证，就可以免费用<a target="_blank" rel="noopener" href="https://github.com/features/copilot/">GitHub Copilot · Your AI pair programmer</a>啦。每个月省了10刀~</li>
</ul>
<p><img src="/2023/02/15/2302151930/1.png"></p>
<ul>
<li>在pycharm中添加插件即可。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">写注释，copilot会自动补全相应的注释和代码。</span><br><span class="line">- tab键应用suggestion（将自动补全的代码，或者根据注释补全的代码应用）</span><br><span class="line">- alt+[或alt+]可以查看其他的suggestion</span><br></pre></td></tr></table></figure>

<ul>
<li>再安利两个我超级喜欢的插件</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个是Indent Rainbow，彩虹缩进🌈，写python超级好用啊！</span><br><span class="line">另外一个是Monokai Pro Theme，我最喜欢的代码配色就是Monokai了！</span><br></pre></td></tr></table></figure>

<p><img src="/2023/02/15/2302151930/2.png"></p>
<blockquote>
<p>使用体验：</p>
<ul>
<li>和ChatGPT相比，Copilot可能更方便辅助日常中的代码构建（尤其是常写的代码，Copilot可以直接内嵌在IDE中，补全代码），可以提高程序员的编码效率~</li>
<li>但是如果要解决实际场景下的编程问题，而不是一些基础的Leetcode算法题或者教学Case，<strong>ChatGPT和Copilot都只是一种辅助工具</strong></li>
<li>距离取代程序员还有很远的距离呢~</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-07T14:05:40.000Z" title="2023/2/7 22:05:40">2023-02-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-02-15T12:22:38.072Z" title="2023/2/15 20:22:38">2023-02-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Util/">Util</a></span><span class="level-item">6 minutes read (About 833 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/07/2302072205/">Util | ChatGPT使用体验以及重拾Hexo遇到的坑</a></p><div class="content"><h1 id="ChatGPT注册"><a href="#ChatGPT注册" class="headerlink" title="ChatGPT注册"></a>ChatGPT注册</h1><ul>
<li>用Microsoft Edge、Google Chrome浏览器。</li>
<li><strong>美区的VPN，开全局模式</strong>。</li>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue (openai.com)</a>👉TRY CHATGPT Button。</li>
<li>如遇到<code>ChatGPT is at capacity right now</code>问题，多刷新几次页面就能进去。</li>
<li>利用Google账号直接登录。</li>
<li>利用<a target="_blank" rel="noopener" href="https://sms-activate.org/">SMS-Activate</a>，下载桌面版软件（网页版做的太烂了），充值0.2刀（支持支付宝），买一个印度区的openai短信服务（最便宜），就可以完成注册了。</li>
<li>如遇到<code>We&#39;re experiencing exceptionally high demand. Please hang tight as we work on scaling our systems.</code>，<code>Too many requests in 1 hour. Try again later.</code>问题<ul>
<li>换梯子、换浏览器，清除cookie和其他站点数据（短时间能解决问题）</li>
<li>花钱买api接口调用（不过马上就要上线付费版的chatgpt pro了？）</li>
<li><strong>尽量白天使用</strong>，晚上别用（用的人太多了，尤其是美国娃，别抢了）</li>
<li>实在不行淘宝等平台买新号用吧。</li>
</ul>
</li>
</ul>
<h1 id="ChatGPT使用体验"><a href="#ChatGPT使用体验" class="headerlink" title="ChatGPT使用体验"></a>ChatGPT使用体验</h1><ul>
<li><p>最初试用ChatGPT时，仅仅让它续写故事，做内容生成。发现是在一本正经的胡说八道，于是就没有再继续使用了。</p>
</li>
<li><p>后来才发现是使用的方法不对。具体可以参考<a target="_blank" rel="noopener" href="https://github.com/f/awesome-chatgpt-prompts">f&#x2F;awesome-chatgpt-prompts: This repo includes ChatGPT prompt curation to use ChatGPT better. (github.com)</a>，里面涉及了ChatGPT可以完成的许多内容。</p>
</li>
<li><p>最近看微软将ChatGPT集成到Bing搜索，以及谷歌等巨头公司做出的一系列举动，发现ChatGPT开启了新的交互式搜索范式，这是让剩下的搜索引擎巨头倍感焦虑的问题。</p>
</li>
<li><p>关于ChatGPT的编码能力，我觉得很适合辅助入门者进行代码的学习，但是复杂一点的问题它就没办法解决了。</p>
</li>
</ul>
<p><img src="/2023/02/07/2302072205/1.png"></p>
<p><img src="/2023/02/07/2302072205/2.png"></p>
<p><img src="/2023/02/07/2302072205/3.png"></p>
<p><img src="/2023/02/07/2302072205/4.png"></p>
<p><img src="/2023/02/07/2302072205/5.png"></p>
<p><img src="/2023/02/07/2302072205/6.png"></p>
<p><img src="/2023/02/07/2302072205/7.png"></p>
<ul>
<li><strong>不管是用YouTube、Google、StackOverflow、GitHub，还是ChatGPT辅助编码，问问题的能力永远比答案更重要，所以搜索引擎是一种工具。</strong>这些工具的诞生能够将简单的、重复性的工作解决掉，所以不愿意终身学习的躺平er们要小心了，在不久的将来，可能就要被代替掉了。</li>
</ul>
<h1 id="Hexo插入图片遇到的坑"><a href="#Hexo插入图片遇到的坑" class="headerlink" title="Hexo插入图片遇到的坑"></a>Hexo插入图片遇到的坑</h1><p>参考</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://segmentfault.com/q/1010000019625231">hexo3 - hexo图片路径问题 - SegmentFault 思否</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/xcodebuild/hexo-asset-image/issues/47">域名是xxx.io的情况下，图片路径会从原本&#x2F;xxx.jpg变成 &#x2F;.io&#x2F;xxx.jpg · Issue #47 · xcodebuild&#x2F;hexo-asset-image (github.com)</a></p>
</li>
</ul>
<p>直接卸载hexo-asset-image插件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-asset-image --save</span><br></pre></td></tr></table></figure>

<p>将_config.yml中</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>md文件中引用格式为</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](<span class="link">同名文章assetfoldername/imagename.png</span>)</span><br></pre></td></tr></table></figure>

<hr>
<p>基于Icarus的一些个人博客：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.wxk.me/about/">about - improveNPC的日志 (wxk.me)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.legendsmb.com/">legendsmb</a></li>
<li><a target="_blank" rel="noopener" href="https://littlezero.top/20190811HexoIc/">Hexo Icarus主题配置完全手册 | 小贪心 (littlezero.top)</a></li>
<li><a target="_blank" rel="noopener" href="https://lqwang.net/">旺阳 (lqwang.net)</a></li>
</ol>
<p>学习文档</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/docs/">文档 | Hexo</a></li>
<li><a target="_blank" rel="noopener" href="https://ppoffice.github.io/hexo-theme-icarus/"><a target="_blank" rel="noopener" href="https://ppoffice.github.io/hexo-theme-icarus/categories/">Categories - Icarus (ppoffice.github.io)</a></a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-icarus">ppoffice&#x2F;hexo-theme-icarus: A simple, delicate, and modern theme for the static site generator Hexo. (github.com)</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-02-07T13:44:23.000Z" title="2023/2/7 21:44:23">2023-02-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-07T12:27:03.029Z" title="2023/3/7 20:27:03">2023-03-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Growth/">Growth</a></span><span class="level-item">17 minutes read (About 2522 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/02/07/2302072144/">Growth | Brand New Start</a></p><div class="content"><blockquote>
<p><strong>写一些废话在前面</strong>：</p>
<p>好久没有写博客了，也很久没有更新个人博客。之前这个站点的内容已经找不到了（大多数是关于深度学习和目标检测的，还有本科做的一些Toy-Project），索性从头配置，把之前的内容全部删除掉了，也算是一个新的开始。</p>
<p>这个博客主题是<strong>ICARUS赛博朋克版本</strong>，决定以后一直用下去了。我笔记本的桌面也是一张赛博朋克风的图，用了很久。最初选择计算机专业，是因为高中时在Coursera自学过前端的课程，当时觉得好有趣，而且我从小就喜欢打游戏;)。后来接触AI，心中便种下了一颗赛博朋克梦，又回到了Coursera，从吴恩达老师的经典《Deep Learning》学起，时常觉得和CS这个学科在冥冥之中有着不解之缘。</p>
<p>虽然大学四年在学业上取得了不错的成绩（刚入学的我数学垫底，及格都会眉飞色舞的开心好一阵，到最后专业第三，还保研到了复旦），可CS毕竟是一门实践学科，四年光阴拿去卷成绩、参加社团活动、各地旅行、认识有趣的人，也算是过的快乐且丰富多彩，但技术上的长进确实不多，背负科班的名号还是有些惭愧的，而成为技术大牛需要夜以继日的持续努力和实践。</p>
<p>22岁，读了研究生，开始认真思考未来的职业规划。</p>
<ul>
<li>是做一些<em>开源项目</em>，<em>背八股刷leetcode</em>，去面试前端后端？（虽然业务场景广泛，绩效清晰，但前端容易被替代，后端技术繁琐，还是要看业务线，以及需求来做事，最终成为螺丝钉，在国内还有裁员风险。）</li>
<li>是赶快<em>水两篇文章</em>，背八股文刷leetcode，去面试算法工程师？（虽然和研究方向沾边，但已经卷成麻花，且没有什么好的业务能落地，移动互联网红利期结束，AI不再像过去一样能圈很多钱，面向落地和前后端没区别，面向算法的炼丹师能让人焦虑的要死，一个月没什么指标提升就可以考虑如何体面的离职了）</li>
<li>是<em>疯狂水文章，读博士</em>，然后再去工业界&#x2F;学术界呢？</li>
<li>是在国内卷，还是<em>润</em>出去呢？</li>
<li>是直接躺平去央企国企银行，考公务员呢？还是继续在大厂卷技术产品经理？</li>
<li>……</li>
</ul>
<p>你看，成长最讨厌的，就是把我们变成现实、唯结果论、喜欢分析利弊的大人。在这个极度内卷的时代，这是生存下去的必要品质，但上述所提到的也不过是一些同质化竞争。我们焦虑，我们张口闭口提八股文leetcode、CCF-A论文×N、50w的package、大厂实习和转正，好像这些是衡量人生成功与否唯一的标志，早就把<strong>热爱</strong>二字抛到九霄云外了，这里的热爱可不是“为的就是这块技术“，现实主义和理想主义从来都不是二选一问题。人变老的标志不是年龄的增长，而是现实消磨掉了你原本拥有的理想主义。</p>
<p>最后再引两段我很喜欢的话，作为过去的结束语，以及未来的开场白：</p>
<p>“那一天我二十一岁，在我一生的黄金时代。我有好多奢望。我想爱，想吃，还想在一瞬间变成天上半明半暗的云。后来我才知道，生活就是个缓慢受锤的过程，人一天天老下去，奢望也一天天消失，最后变得像挨了锤的牛一样。可是我过二十一岁生日时没有预见到这一点。<strong>我觉得自己会永远生猛下去，什么也锤不了我。</strong>“</p>
<p>“回头看他人生的故事的时候，我们或许觉得传奇精彩，可是带入来看却会发现他在面临每一个人生岔路口时，做出的每一个决定需要付出多大的勇气。<strong>都说人生任何时候开始都不晚，或许前提是那个人背负着巨大的足以支撑其走到最后的信念和梦想。毕竟人生区区百年，只争朝夕。</strong>“</p>
<p>2023.02.07 于广州。</p>
<hr>
<p>更新想法。</p>
<p>任何技术（前端&#x2F;后端&#x2F;算法&#x2F;数据库&#x2F;网安&#x2F;操作系统）都可以深挖，都需要持续不断的学习，才不会被日新月异的技术洪流淘汰掉。找到自己感兴趣的就好。</p>
<p>以及，</p>
<p>Try first. 任何新鲜事物都先试试看。</p>
<p><strong>大凡事物必有顺序。</strong></p>
<ul>
<li><strong>看得太超前了不行，看得太超前，势必忽视脚下，人往往跌倒。</strong></li>
<li><strong>可另一方面，光看脚下也不行，不知道前面你会撞上什么。</strong></li>
<li><strong>所以，要在适度往前看的同时，按部就班处理眼下事物。这点至为关键，无论做什么。</strong></li>
</ul>
<p><strong>——《海边的卡夫卡》</strong></p>
<p>2023.02.08 于广州。</p>
<hr>
<p>更新想法。</p>
<p>昨天晚上的飞机，终于回到了上海。</p>
<p>今天收拾了一下行李，晚上骑着我的可爱电瓶车就来实验室了（感觉车好像坏掉了，明天一定要记得去修啊！），呆在实验室心情开心的爆表了！我真是骨子里的工作狂…（也不是啦，天天在家躺着就觉得好丧，如果不做些自己觉得有意义的事情，就会觉得出去玩也不快乐…）</p>
<p>昨晚躺床上，和舍友聊职业规划、未来blahblah的。</p>
<p>舍友：chatgpt出来，还是觉得后端更容易被代替（因为我说前端更容易被替代），虽然也有难的技术部分（比如处理高并发场景，还有许多涉及复杂算法的场景），但大多数情况下，日常工作中就是增删改查crud。前端可能会好点，因为需要人觉得美观，而且科班出来一般都是做后端，竞争更激烈。（but我觉得前端就是拿产品+设计给的图，照着敲罢了…因为之前试着做过一段前端的工作，其实美观不美观是设计需要处理的，设计没准在不远的将来也会被代替了，更别说前端了。）</p>
<p>舍友：居然是nlp先出现了这种模型，那cv呢？（我：我觉得可能cv要做这种多任务大预训练太费算力了。。所以目前没有人那么有钱敢这么做的哈哈哈哈。不过我估计<strong>未来都是大模型的天下（算力、数据方面）</strong>了。要么就是<strong>等一个真正的底层颠覆性的DL算法创新（算法方面）</strong>）</p>
<p>最后我俩总结，【如果想搞钱的话】（这是前提，今天刷到一句话，”Ta聊起研究方向时能够滔滔不绝，Ta眼里闪闪发光，说读博的目的是为了尝试突破某个方向的科研桎梏“。我想，这才是一个真正的博士该有的品质吧，也蛮像我自己理想中的生活的。其实感觉自己还是蛮有科研激情的，不过自己的性格是真的坐不住静不下来）</p>
<ul>
<li><p>前后端等等容易被代替（聊了一下chatgpt、以及微软和openai合作的GitHub Copilot辅助写代码很好用）。pass掉做软件产品、工程向。</p>
</li>
<li><p>算法研究岗要读个博士，预计未来DL都是大模型的天下，高校也没有算力支撑做这些事情，除非和企业合作。pass掉工业&#x2F;学术界科研向。</p>
</li>
<li><p>硕士出去肯定做不了research。算法工程落地写python c++，处理数据。勉强能冲着看。但要是炼丹提指标是kpi，那可能也不是很友好。</p>
</li>
<li><p>搞一搞AI+金融（我俩对金融都有点兴趣），找个类似私募的单位做点东西。我觉得这条路还挺有意思的。但是转纯金融是不可能的，马太效应最明显的一个行业。</p>
</li>
</ul>
<p>最简单的还是去找一些工作几年了的前辈聊一聊，感觉会少走很多的弯路。而且我觉得写代码只是程序员、研究员工作中最基础的一个环节，其实也没必要过于悲观。而且<strong>从商业角度看，软件技术，主要靠一个商业模式，要有一个好的故事，要有用户肯买账才行</strong>。</p>
<p>今天又是没想清楚未来该何去何从的一天呢！</p>
<p>不过能够肯定的是，以后一定要做<strong>计算机&#x2F;金融</strong>这俩方向的。</p>
<p>先把手头的事情做好，船到桥头自然直~</p>
<p>2023.02.13 于上海。</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-01-04T12:44:04.000Z" title="2023/1/4 20:44:04">2023-01-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-03-07T12:45:56.148Z" title="2023/3/7 20:45:56">2023-03-07</time></span><span class="level-item">2 minutes read (About 262 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/04/230104/">Util | Git使用小结</a></p><div class="content"><blockquote>
<p>从今天开始正式使用git，记录每日代码量，早日成为5w行入门码农。<br><strong>Talk is cheap. Show me the code.</strong></p>
</blockquote>
<h1 id="1-Git-提交代码"><a href="#1-Git-提交代码" class="headerlink" title="1 Git 提交代码"></a>1 Git 提交代码</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. git status # 查看当前状态、文件修改状态（红字）</span><br><span class="line">2. git add . or git add &lt;xxx（文件路径+文件名）&gt; # 添加内容到本地git缓存区</span><br><span class="line">3. git commit -m &quot;&lt;备注&gt;&quot; # 推送修改到本地git库</span><br><span class="line">4. git pull &lt;远程主机名&gt;&lt;远程分支名&gt; # 取回远程主机某个分支的更新，与本地指定分支合并</span><br><span class="line">5. git push &lt;远程主机名&gt;&lt;远程分支名&gt; # 本地仓库代码推送到远程主机的某个远程分支上</span><br></pre></td></tr></table></figure>

<h1 id="2-Git-代码管理"><a href="#2-Git-代码管理" class="headerlink" title="2 Git 代码管理"></a>2 Git 代码管理</h1><ol>
<li>个人代码量统计</li>
</ol>
<ul>
<li>全部时间<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --author=&quot;&lt;username&gt;&quot; --pretty=tformat: --numstat | awk &#x27;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc &#125;&#x27; -</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/8a21afe9bdab4cacb0675cbc8d2f7431.png" alt="在这里插入图片描述"></li>
<li>指定时间段<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git log --since=&quot;&lt;start date e.g.2020-01-01&gt;&quot; --before=&quot;&lt;end date e.g.2020-12-31&gt;&quot; --author=&quot;&lt;username&gt;&quot; \</span><br><span class="line">--pretty=tformat: --numstat | awk &#x27;&#123; add += $1; subs += $2; loc += $1 - $2 &#125; END &#123; printf &quot;新增行数: %s, 移除行数: %s, 总行数: %s\n&quot;, add, subs, loc &#125;&#x27;</span><br></pre></td></tr></table></figure>
<img src="https://img-blog.csdnimg.cn/aad65a2f82624ecf81ba0452691571ac.png" alt="在这里插入图片描述" style="zoom:200%;" /></li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar1.png" alt="Vanessa Ni🛸🏴‍☠️"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Vanessa Ni🛸🏴‍☠️</p><p class="is-size-6 is-block">💗I am currently a first-year postgraduate student at Fudan University. My research focuses on computer vision👁️, especially image/face inpainting and image quality assessment. 💻I graduated from Tianjin University with a Bachelor&#039;s degree in Software Engineering, and acquired a little knowledge about 2d/3d object detection. I worked as an algorithm engineer intern at CATRC and Baidu. 📙I enjoy reading (especially psychology and philosophy), traveling🧳, photography📸, working out💪, and dancing💃 in my free time. 🚀I want to become an algorithm engineer or go to the U.S. for Ph.D. 3 years later.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JennyVanessa" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JennyVanessa"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Growth/"><span class="level-start"><span class="level-item">Growth</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Util/"><span class="level-start"><span class="level-item">Util</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T08:16:52.000Z">2023-04-04</time></p><p class="title"><a href="/2023/04/04/2304041616/">Paper | Perceptual Artifacts Localization for Inpainting | ECCV2022</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-04T08:11:21.000Z">2023-04-04</time></p><p class="title"><a href="/2023/04/04/2304041611/">Paper | High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling | ECCV2020</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-27T09:23:48.000Z">2023-03-27</time></p><p class="title"><a href="/2023/03/27/2303271723/">Paper | SmartBrush, Text and Shape Guided Object Inpainting with Diffusion Model | CVPR2023</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-25T14:16:24.000Z">2023-03-25</time></p><p class="title"><a href="/2023/03/25/2303252216/">Paper | Generative Image Inpainting with Segmentation Confusion Adversarial Training and Contrastive Learning | AAAI2023</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-20T11:28:02.000Z">2023-03-20</time></p><p class="title"><a href="/2023/03/20/2303201927/">Paper | CoordFill, Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying | AAAI2023</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Growth/"><span class="tag">Growth</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/paper/"><span class="tag">paper</span><span class="tag">11</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Just be here now🎇</a><p class="is-size-7"><span>&copy; 2023 Vanessa Ni</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>